"use strict";(globalThis.webpackChunk=globalThis.webpackChunk||[]).push([[2300],{5656:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>c,contentTitle:()=>l,default:()=>d,frontMatter:()=>s,metadata:()=>t,toc:()=>r});const t=JSON.parse('{"id":"modules/module-4-vision-language-action/capstone-project","title":"Capstone Project - The Autonomous Humanoid","description":"Complete VLA pipeline demonstration showing how a simulated humanoid robot receives voice commands, plans paths, navigates obstacles, identifies objects, and manipulates them.","source":"@site/docs/modules/module-4-vision-language-action/capstone-project.md","sourceDirName":"modules/module-4-vision-language-action","slug":"/modules/module-4-vision-language-action/capstone-project","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-4-vision-language-action/capstone-project","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"vla","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/vla"},{"inline":true,"label":"capstone","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/capstone"},{"inline":true,"label":"autonomous-robot","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/autonomous-robot"},{"inline":true,"label":"voice-to-action","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/voice-to-action"},{"inline":true,"label":"cognitive-planning","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/cognitive-planning"},{"inline":true,"label":"navigation","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/navigation"},{"inline":true,"label":"manipulation","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/manipulation"},{"inline":true,"label":"humanoid-robotics","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/humanoid-robotics"}],"version":"current","sidebarPosition":6,"frontMatter":{"id":"capstone-project","title":"Capstone Project - The Autonomous Humanoid","sidebar_position":6,"description":"Complete VLA pipeline demonstration showing how a simulated humanoid robot receives voice commands, plans paths, navigates obstacles, identifies objects, and manipulates them.","tags":["vla","capstone","autonomous-robot","voice-to-action","cognitive-planning","navigation","manipulation","humanoid-robotics"],"learning_objectives":["lo-013"]},"sidebar":"textbookSidebar","previous":{"title":"Safety & Validation of LLM-Generated Plans","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-4-vision-language-action/safety-validation"},"next":{"title":"Module Integration - Connecting VLA to Previous Modules","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-4-vision-language-action/module-integration"}}');var o=i(4848),a=i(8453);const s={id:"capstone-project",title:"Capstone Project - The Autonomous Humanoid",sidebar_position:6,description:"Complete VLA pipeline demonstration showing how a simulated humanoid robot receives voice commands, plans paths, navigates obstacles, identifies objects, and manipulates them.",tags:["vla","capstone","autonomous-robot","voice-to-action","cognitive-planning","navigation","manipulation","humanoid-robotics"],learning_objectives:["lo-013"]},l="Capstone Project: The Autonomous Humanoid",c={},r=[{value:"Project Overview",id:"project-overview",level:2},{value:"The Complete VLA Pipeline",id:"the-complete-vla-pipeline",level:2},{value:"Step-by-Step Flow",id:"step-by-step-flow",level:2},{value:"Step 1: Voice Command",id:"step-1-voice-command",level:3},{value:"Step 2: Cognitive Planning",id:"step-2-cognitive-planning",level:3},{value:"Step 3: Path Planning",id:"step-3-path-planning",level:3},{value:"Step 4: Obstacle Navigation",id:"step-4-obstacle-navigation",level:3},{value:"Step 5: Object Identification",id:"step-5-object-identification",level:3},{value:"Step 6: Object Manipulation",id:"step-6-object-manipulation",level:3},{value:"Integration of VLA Components",id:"integration-of-vla-components",level:2},{value:"Voice Input Integration",id:"voice-input-integration",level:3},{value:"Cognitive Planning Integration",id:"cognitive-planning-integration",level:3},{value:"ROS 2 Action Generation Integration",id:"ros-2-action-generation-integration",level:3},{value:"Cross-References to Previous Modules",id:"cross-references-to-previous-modules",level:2},{value:"Module 2: Simulation",id:"module-2-simulation",level:3},{value:"Module 3: Perception and Navigation",id:"module-3-perception-and-navigation",level:3},{value:"Complete Integration Flow Diagram",id:"complete-integration-flow-diagram",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function p(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"capstone-project-the-autonomous-humanoid",children:"Capstone Project: The Autonomous Humanoid"})}),"\n",(0,o.jsx)(e.p,{children:"This capstone project demonstrates the complete Vision-Language-Action (VLA) pipeline in action. You'll see how a simulated humanoid robot receives a voice command, uses cognitive planning to generate a path, navigates obstacles using perception, identifies objects using computer vision, and manipulates them\u2014all integrated into a cohesive autonomous behavior."}),"\n",(0,o.jsx)(e.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,o.jsx)(e.p,{children:"The capstone project integrates all VLA concepts into a single demonstration:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Voice input"}),": A spoken command initiates the autonomous behavior"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Cognitive planning"}),": Natural language is translated into an action plan"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Path planning"}),": A navigation path is generated to reach the goal"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Obstacle navigation"}),": The robot navigates around obstacles using perception"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Object identification"}),": Computer vision identifies target objects"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Object manipulation"}),": The robot grasps and manipulates objects"]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"This project shows how voice-to-action, cognitive planning, perception, navigation, and manipulation work together in a complete VLA system."}),"\n",(0,o.jsx)(e.h2,{id:"the-complete-vla-pipeline",children:"The Complete VLA Pipeline"}),"\n",(0,o.jsx)(e.p,{children:"The capstone demonstrates the end-to-end flow:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"Voice Command \u2192 Speech Recognition \u2192 Cognitive Planning \u2192 \nPath Planning \u2192 Obstacle Navigation \u2192 Object Identification \u2192 \nObject Manipulation \u2192 Task Completion\n"})}),"\n",(0,o.jsx)(e.p,{children:"Each stage builds upon the previous, creating a seamless autonomous behavior."}),"\n",(0,o.jsx)(e.h2,{id:"step-by-step-flow",children:"Step-by-Step Flow"}),"\n",(0,o.jsx)(e.h3,{id:"step-1-voice-command",children:"Step 1: Voice Command"}),"\n",(0,o.jsxs)(e.p,{children:["The user speaks a command: ",(0,o.jsx)(e.strong,{children:'"Pick up the red cup from the table"'})]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Audio capture"}),": Microphones capture the spoken command"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Speech recognition"}),": OpenAI Whisper transcribes speech to text"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Text output"}),': "Pick up the red cup from the table"']}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"step-2-cognitive-planning",children:"Step 2: Cognitive Planning"}),"\n",(0,o.jsx)(e.p,{children:"The transcribed text is processed by cognitive planning:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Intent understanding"}),": The robot understands it needs to pick up a specific object"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Goal decomposition"}),": The task is broken into sub-goals:","\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Navigate to the table"}),"\n",(0,o.jsx)(e.li,{children:"Identify the red cup"}),"\n",(0,o.jsx)(e.li,{children:"Pick up the cup"}),"\n",(0,o.jsx)(e.li,{children:"Verify successful grasp"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action sequence generation"}),": ROS 2 actions are generated for each sub-goal"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"step-3-path-planning",children:"Step 3: Path Planning"}),"\n",(0,o.jsx)(e.p,{children:"The cognitive plan includes navigation to the table:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Target location"}),": The table location is identified"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Path generation"}),": A path is planned from current position to table"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Obstacle consideration"}),": Known obstacles are considered in path planning"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Path validation"}),": The path is checked for feasibility"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"step-4-obstacle-navigation",children:"Step 4: Obstacle Navigation"}),"\n",(0,o.jsx)(e.p,{children:"The robot navigates to the table:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Perception"}),": Sensors detect obstacles in the environment"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Path adjustment"}),": The path is adjusted to avoid obstacles"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Navigation execution"}),": ROS 2 navigation actions move the robot"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Progress monitoring"}),": Navigation progress is monitored and adjusted"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"step-5-object-identification",children:"Step 5: Object Identification"}),"\n",(0,o.jsx)(e.p,{children:"At the table, computer vision identifies the target:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Visual perception"}),": Cameras capture images of the table"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Object detection"}),": Computer vision detects objects on the table"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Object classification"}),": Objects are classified (cup, bottle, etc.)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Target selection"}),": The red cup is identified and selected"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"step-6-object-manipulation",children:"Step 6: Object Manipulation"}),"\n",(0,o.jsx)(e.p,{children:"The robot manipulates the identified object:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Grasp planning"}),": A grasp pose is planned for the cup"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Arm movement"}),": The robot's arm moves to the grasp pose"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Grasp execution"}),": The robot grasps the cup"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Verification"}),": Successful grasp is verified"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"integration-of-vla-components",children:"Integration of VLA Components"}),"\n",(0,o.jsx)(e.p,{children:"This capstone demonstrates how all VLA components integrate:"}),"\n",(0,o.jsx)(e.h3,{id:"voice-input-integration",children:"Voice Input Integration"}),"\n",(0,o.jsx)(e.p,{children:"Voice commands initiate the entire pipeline:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# Whisper Integration Pattern in Capstone Context\n# This demonstrates how voice input initiates the capstone project\n\nimport whisper\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass CapstoneVoiceInput(Node):\n    """\n    Voice input component for capstone project.\n    Demonstrates Whisper integration in complete VLA pipeline.\n    """\n    \n    def __init__(self):\n        super().__init__(\'capstone_voice_input\')\n        \n        # Initialize Whisper (conceptual pattern)\n        self.whisper_model = whisper.load_model("base")\n        \n        # Publisher for sending commands to cognitive planning\n        self.command_publisher = self.create_publisher(\n            String,\n            \'voice_command\',\n            10\n        )\n    \n    def process_voice_command(self, audio_data):\n        """\n        Process voice command and publish to cognitive planning.\n        This initiates the complete VLA pipeline.\n        """\n        # Transcribe audio to text using Whisper\n        result = self.whisper_model.transcribe(audio_data)\n        command_text = result["text"]\n        \n        # Publish command to cognitive planning system\n        msg = String()\n        msg.data = command_text\n        self.command_publisher.publish(msg)\n        \n        self.get_logger().info(f\'Voice command received: {command_text}\')\n        return command_text\n'})}),"\n",(0,o.jsx)(e.h3,{id:"cognitive-planning-integration",children:"Cognitive Planning Integration"}),"\n",(0,o.jsx)(e.p,{children:"Cognitive planning translates commands to action sequences:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# LLM Cognitive Planning Integration Pattern in Capstone Context\n# This demonstrates how cognitive planning generates action sequences\n\nclass CapstoneCognitivePlanner:\n    """\n    Cognitive planning component for capstone project.\n    Demonstrates LLM integration in complete VLA pipeline.\n    """\n    \n    def __init__(self, llm_client):\n        self.llm_client = llm_client\n    \n    def plan_capstone_task(self, voice_command, robot_context):\n        """\n        Generate action plan for capstone task.\n        This demonstrates cognitive planning in complete VLA pipeline.\n        """\n        # Create planning prompt\n        prompt = f"""\n        Translate this voice command into a robot action plan:\n        Command: "{voice_command}"\n        \n        Robot context: {robot_context}\n        \n        Generate a sequence of actions:\n        1. Navigation to target location\n        2. Object identification\n        3. Object manipulation\n        \n        Output the action sequence in ROS 2 action format.\n        """\n        \n        # Generate plan using LLM\n        response = self.llm_client.generate(prompt)\n        \n        # Parse response into action sequence\n        action_sequence = self.parse_plan(response)\n        \n        return action_sequence\n    \n    def parse_plan(self, llm_response):\n        """\n        Parse LLM response into ROS 2 action sequence.\n        This demonstrates how cognitive plans become executable actions.\n        """\n        # Parse structured response\n        # Extract navigation, perception, and manipulation actions\n        actions = []\n        # ... parsing logic ...\n        return actions\n'})}),"\n",(0,o.jsx)(e.h3,{id:"ros-2-action-generation-integration",children:"ROS 2 Action Generation Integration"}),"\n",(0,o.jsx)(e.p,{children:"ROS 2 actions execute the cognitive plan:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# ROS 2 Action Generation Integration Pattern in Capstone Context\n# This demonstrates how cognitive plans become ROS 2 actions\n\nfrom rclpy.action import ActionClient\nfrom navigation_msgs.action import NavigateToPose\nfrom manipulation_msgs.action import PickPlace\nfrom perception_msgs.action import DetectObjects\n\nclass CapstoneActionExecutor(Node):\n    """\n    Action execution component for capstone project.\n    Demonstrates ROS 2 action generation in complete VLA pipeline.\n    """\n    \n    def __init__(self):\n        super().__init__(\'capstone_action_executor\')\n        \n        # Action clients for different capabilities\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n        self.manip_client = ActionClient(self, PickPlace, \'pick_place\')\n        self.perception_client = ActionClient(self, DetectObjects, \'detect_objects\')\n    \n    def execute_capstone_plan(self, cognitive_plan):\n        """\n        Execute complete capstone plan.\n        This demonstrates ROS 2 action execution in complete VLA pipeline.\n        """\n        # Execute navigation actions\n        for nav_action in cognitive_plan.navigation_actions:\n            self.execute_navigation(nav_action)\n        \n        # Execute perception actions\n        for perception_action in cognitive_plan.perception_actions:\n            self.execute_perception(perception_action)\n        \n        # Execute manipulation actions\n        for manip_action in cognitive_plan.manipulation_actions:\n            self.execute_manipulation(manip_action)\n    \n    def execute_navigation(self, action):\n        """Execute navigation action."""\n        goal = NavigateToPose.Goal()\n        goal.pose = action.target_pose\n        self.nav_client.send_goal_async(goal)\n    \n    def execute_manipulation(self, action):\n        """Execute manipulation action."""\n        goal = PickPlace.Goal()\n        goal.object_id = action.object_id\n        goal.pick_pose = action.pick_pose\n        self.manip_client.send_goal_async(goal)\n'})}),"\n",(0,o.jsx)(e.h2,{id:"cross-references-to-previous-modules",children:"Cross-References to Previous Modules"}),"\n",(0,o.jsx)(e.p,{children:"This capstone project builds upon concepts from previous modules:"}),"\n",(0,o.jsx)(e.h3,{id:"module-2-simulation",children:"Module 2: Simulation"}),"\n",(0,o.jsxs)(e.p,{children:["The capstone project runs in a simulated environment, demonstrating how ",(0,o.jsx)(e.a,{href:"/modules/module-2-digital-twins-simulation/simulation-fundamentals",children:"Module 2: Digital Twins - Simulation & Sensors"})," enables safe testing and development of VLA systems. Simulation allows:"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Safe testing"}),": Testing VLA systems without physical risk"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Rapid iteration"}),": Quickly testing different scenarios"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Environment control"}),": Creating consistent test conditions"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"module-3-perception-and-navigation",children:"Module 3: Perception and Navigation"}),"\n",(0,o.jsxs)(e.p,{children:["The capstone project uses perception and navigation capabilities from ",(0,o.jsx)(e.a,{href:"/modules/module-3-ai-robot-brain/nav2-path-planning",children:"Module 3: The AI-Robot Brain (NVIDIA Isaac\u2122)"}),", including:"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Computer vision"}),": Object identification and classification"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Path planning"}),": Navigation path generation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Obstacle avoidance"}),": Dynamic obstacle navigation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Sensor integration"}),": Using cameras and sensors for perception"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"complete-integration-flow-diagram",children:"Complete Integration Flow Diagram"}),"\n",(0,o.jsx)(e.p,{children:"The following diagram illustrates the complete capstone project flow:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-mermaid",children:"flowchart TD\n    A[Voice Command] --\x3e B[Whisper Transcription]\n    B --\x3e C[Cognitive Planning]\n    C --\x3e D[Action Sequence]\n    D --\x3e E[Path Planning]\n    E --\x3e F[Navigation Actions]\n    F --\x3e G[Obstacle Navigation]\n    G --\x3e H[Object Detection]\n    H --\x3e I[Object Identification]\n    I --\x3e J[Grasp Planning]\n    J --\x3e K[Manipulation Actions]\n    K --\x3e L[Task Completion]\n    \n    style A fill:#e1f5ff\n    style C fill:#fff4e1\n    style E fill:#e8f5e9\n    style L fill:#fce4ec\n"})}),"\n",(0,o.jsx)(e.p,{children:"This diagram shows how all VLA components integrate in the complete autonomous behavior."}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"The capstone project demonstrates the complete VLA pipeline, integrating voice input, cognitive planning, path planning, obstacle navigation, object identification, and manipulation into a cohesive autonomous behavior. This project shows how all VLA concepts work together, from a simple voice command to complex robot behavior. Understanding this integration is essential for comprehending how VLA systems enable natural language robot control."}),"\n",(0,o.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(e.p,{children:["Now that you understand the complete VLA pipeline through the capstone project, proceed to ",(0,o.jsx)(e.a,{href:"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-4-vision-language-action/module-integration",children:"Module Integration"})," to learn how VLA concepts connect to and build upon previous modules."]})]})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(p,{...n})}):p(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>s,x:()=>l});var t=i(6540);const o={},a=t.createContext(o);function s(n){const e=t.useContext(a);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),t.createElement(a.Provider,{value:e},n.children)}}}]);