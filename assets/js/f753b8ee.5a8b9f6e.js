"use strict";(globalThis.webpackChunk=globalThis.webpackChunk||[]).push([[9082],{5317:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>r,contentTitle:()=>c,default:()=>g,frontMatter:()=>a,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"modules/module-4-vision-language-action/glossary","title":"Glossary - Key Terminology","description":"Key terminology definitions for Module 4: Vision-Language-Action (VLA), including VLA, voice-to-action, cognitive planning, and related concepts.","source":"@site/docs/modules/module-4-vision-language-action/glossary.md","sourceDirName":"modules/module-4-vision-language-action","slug":"/modules/module-4-vision-language-action/glossary","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-4-vision-language-action/glossary","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"vla","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/vla"},{"inline":true,"label":"glossary","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/glossary"},{"inline":true,"label":"terminology","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/terminology"},{"inline":true,"label":"definitions","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/definitions"}],"version":"current","sidebarPosition":8,"frontMatter":{"id":"glossary","title":"Glossary - Key Terminology","sidebar_position":8,"description":"Key terminology definitions for Module 4: Vision-Language-Action (VLA), including VLA, voice-to-action, cognitive planning, and related concepts.","tags":["vla","glossary","terminology","definitions"]},"sidebar":"textbookSidebar","previous":{"title":"Module Integration - Connecting VLA to Previous Modules","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-4-vision-language-action/module-integration"}}');var o=i(4848),s=i(8453);const a={id:"glossary",title:"Glossary - Key Terminology",sidebar_position:8,description:"Key terminology definitions for Module 4: Vision-Language-Action (VLA), including VLA, voice-to-action, cognitive planning, and related concepts.",tags:["vla","glossary","terminology","definitions"]},c="Glossary: Key Terminology",r={},l=[{value:"Vision-Language-Action (VLA)",id:"vision-language-action-vla",level:2},{value:"Voice-to-Action",id:"voice-to-action",level:2},{value:"Cognitive Planning",id:"cognitive-planning",level:2},{value:"Natural Language Intent",id:"natural-language-intent",level:2},{value:"Action Sequence",id:"action-sequence",level:2},{value:"VLA Pipeline",id:"vla-pipeline",level:2},{value:"Cognitive Plan",id:"cognitive-plan",level:2},{value:"Voice Command",id:"voice-command",level:2},{value:"Action Sequence",id:"action-sequence-1",level:2},{value:"Capstone Project Scenario",id:"capstone-project-scenario",level:2}];function d(n){const e={h1:"h1",h2:"h2",header:"header",p:"p",strong:"strong",...(0,s.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"glossary-key-terminology",children:"Glossary: Key Terminology"})}),"\n",(0,o.jsx)(e.p,{children:"This glossary defines key terms used throughout Module 4: Vision-Language-Action (VLA)."}),"\n",(0,o.jsx)(e.h2,{id:"vision-language-action-vla",children:"Vision-Language-Action (VLA)"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Definition"}),": A unified framework that combines vision, language processing, and action execution to enable natural language interaction with robots."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Context"}),": VLA systems enable robots to understand spoken or written commands, perceive their environment, and execute physical actions based on language instructions."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Related Terms"}),": Cognitive planning, voice-to-action, natural language processing"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example"}),': A VLA system allows a user to say "Pick up the red cup" and the robot understands the command, identifies the cup, and picks it up.']}),"\n",(0,o.jsx)(e.h2,{id:"voice-to-action",children:"Voice-to-Action"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Definition"}),": The capability that enables robots to convert spoken commands into robot actions through speech recognition and cognitive planning."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Context"}),": Voice-to-action systems use speech recognition (like OpenAI Whisper) to transcribe spoken commands, which are then processed by cognitive planning systems to generate robot actions."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Related Terms"}),": Speech recognition, cognitive planning, natural language processing"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example"}),': When a user says "Clean the room," a voice-to-action system transcribes the speech and generates a sequence of robot actions to accomplish the task.']}),"\n",(0,o.jsx)(e.h2,{id:"cognitive-planning",children:"Cognitive Planning"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Definition"}),": The process by which Large Language Models (LLMs) translate natural language commands into sequences of executable robot actions."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Context"}),": Cognitive planning decomposes high-level natural language instructions into structured action plans that robots can execute through ROS 2 actions."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Related Terms"}),": LLM, natural language processing, action sequence, ROS 2"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example"}),': Cognitive planning translates "Clean the room" into a sequence of navigation, perception, and manipulation actions.']}),"\n",(0,o.jsx)(e.h2,{id:"natural-language-intent",children:"Natural Language Intent"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Definition"}),": The semantic meaning and goal extracted from a voice command or text instruction."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Context"}),": Natural language intent represents what the user wants the robot to accomplish, including the goal, required capabilities, constraints, and context."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Related Terms"}),": Intent understanding, goal decomposition, cognitive planning"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example"}),': The natural language intent of "Pick up the red cup" includes the goal (pick up), the target (red cup), and implicit constraints (use appropriate grasp).']}),"\n",(0,o.jsx)(e.h2,{id:"action-sequence",children:"Action Sequence"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Definition"}),": An ordered list of ROS 2 actions that implement a cognitive plan."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Context"}),": Action sequences represent the executable robot behaviors generated by cognitive planning, including individual actions, action parameters, dependencies, and execution order."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Related Terms"}),": ROS 2 actions, cognitive plan, action execution"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example"}),': An action sequence for "Pick up the cup" might include: navigate to table, detect objects, identify cup, plan grasp, execute grasp.']}),"\n",(0,o.jsx)(e.h2,{id:"vla-pipeline",children:"VLA Pipeline"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Definition"}),": The complete flow from voice input to physical action in VLA systems."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Context"}),": The VLA pipeline includes voice capture, speech recognition, text transcription, cognitive planning, action generation, perception, navigation, and manipulation stages."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Related Terms"}),": Voice-to-action, cognitive planning, action execution"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example"}),": The VLA pipeline flows: Voice command \u2192 Whisper transcription \u2192 Cognitive planning \u2192 ROS 2 actions \u2192 Robot execution."]}),"\n",(0,o.jsx)(e.h2,{id:"cognitive-plan",children:"Cognitive Plan"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Definition"}),": A structured sequence of robot actions generated by an LLM from natural language input."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Context"}),": Cognitive plans bridge natural language intent to executable robot behaviors, containing high-level goals, decomposed sub-tasks, action sequences, and execution parameters."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Related Terms"}),": Cognitive planning, action sequence, natural language intent"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example"}),': A cognitive plan for "Clean the room" includes sub-tasks like navigate, identify objects, pick up objects, and place objects, each with associated ROS 2 actions.']}),"\n",(0,o.jsx)(e.h2,{id:"voice-command",children:"Voice Command"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Definition"}),": A spoken instruction given to a humanoid robot."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Context"}),": Voice commands contain audio waveform data, transcribed text, semantic meaning, and intent, enabling natural language human-robot interaction."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Related Terms"}),": Voice-to-action, speech recognition, natural language intent"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example"}),': "Pick up the red cup from the table" is a voice command that initiates the VLA pipeline.']}),"\n",(0,o.jsx)(e.h2,{id:"action-sequence-1",children:"Action Sequence"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Definition"}),": An ordered list of ROS 2 actions that implement a cognitive plan."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Context"}),": Action sequences contain individual actions (navigation, manipulation, perception), action parameters, dependencies between actions, and execution order."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Related Terms"}),": ROS 2 actions, cognitive plan, action execution"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example"}),": An action sequence might include: NavigateToPose, DetectObjects, PickPlace actions in a specific order."]}),"\n",(0,o.jsx)(e.h2,{id:"capstone-project-scenario",children:"Capstone Project Scenario"}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Definition"}),": A complete autonomous behavior demonstration integrating all VLA components."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Context"}),": Capstone project scenarios demonstrate practical application of VLA concepts, including voice command input, planning phase, navigation phase, object identification phase, and manipulation phase."]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Related Terms"}),": VLA pipeline, autonomous behavior, integration"]}),"\n",(0,o.jsxs)(e.p,{children:[(0,o.jsx)(e.strong,{children:"Example"}),": The capstone project demonstrates a complete scenario where a robot receives a voice command, plans a path, navigates obstacles, identifies objects, and manipulates them."]})]})}function g(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(d,{...n})}):d(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>c});var t=i(6540);const o={},s=t.createContext(o);function a(n){const e=t.useContext(s);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function c(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:a(n.components),t.createElement(s.Provider,{value:e},n.children)}}}]);