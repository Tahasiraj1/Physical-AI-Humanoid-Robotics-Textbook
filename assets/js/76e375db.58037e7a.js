"use strict";(globalThis.webpackChunk=globalThis.webpackChunk||[]).push([[3730],{8254:i=>{i.exports=JSON.parse('{"tag":{"label":"cognitive-planning","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/cognitive-planning","allTagsPath":"/Physical-AI-Humanoid-Robotics-Textbook/tags","count":4,"items":[{"id":"modules/module-4-vision-language-action/capstone-project","title":"Capstone Project - The Autonomous Humanoid","description":"Complete VLA pipeline demonstration showing how a simulated humanoid robot receives voice commands, plans paths, navigates obstacles, identifies objects, and manipulates them.","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-4-vision-language-action/capstone-project"},{"id":"modules/module-4-vision-language-action/cognitive-planning","title":"Cognitive Planning with LLMs","description":"Understanding how LLMs perform cognitive planning by translating natural language commands into ROS 2 action sequences.","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-4-vision-language-action/cognitive-planning"},{"id":"modules/module-4-vision-language-action/module-4-vision-language-action","title":"Module 4 - Vision-Language-Action (VLA)","description":"Introduction to Vision-Language-Action (VLA) systems, covering LLM-robotics convergence, voice-to-action, cognitive planning, and complete VLA pipeline integration.","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-4-vision-language-action/"},{"id":"modules/module-4-vision-language-action/safety-validation","title":"Safety & Validation of LLM-Generated Plans","description":"Understanding how LLM-generated action plans are validated and executed safely, including plan verification and constraint checking.","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-4-vision-language-action/safety-validation"}],"unlisted":false}}')}}]);