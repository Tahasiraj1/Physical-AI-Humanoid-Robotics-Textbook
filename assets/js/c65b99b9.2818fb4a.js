"use strict";(globalThis.webpackChunk=globalThis.webpackChunk||[]).push([[2884],{4072:o=>{o.exports=JSON.parse('{"tag":{"label":"humanoid-robotics","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/humanoid-robotics","allTagsPath":"/Physical-AI-Humanoid-Robotics-Textbook/tags","count":13,"items":[{"id":"modules/module-4-vision-language-action/capstone-project","title":"Capstone Project - The Autonomous Humanoid","description":"Complete VLA pipeline demonstration showing how a simulated humanoid robot receives voice commands, plans paths, navigates obstacles, identifies objects, and manipulates them.","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-4-vision-language-action/capstone-project"},{"id":"modules/module-4-vision-language-action/cognitive-planning","title":"Cognitive Planning with LLMs","description":"Understanding how LLMs perform cognitive planning by translating natural language commands into ROS 2 action sequences.","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-4-vision-language-action/cognitive-planning"},{"id":"modules/module-2-digital-twins-simulation/humanoid-applications","title":"Humanoid Applications","description":"Practical use cases for digital twins in humanoid robotics, including gait optimization, manipulation planning, and safety testing.","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-2-digital-twins-simulation/humanoid-applications"},{"id":"modules/module-1-ros2-nervous-system/humanoid-applications","title":"Humanoid Robotics Applications","description":"Connecting ROS 2 concepts to humanoid robotics applications including sensor integration, actuator coordination, and locomotion control.","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-1-ros2-nervous-system/humanoid-applications"},{"id":"modules/module-2-digital-twins-simulation/introduction","title":"Introduction - Digital Twins, Simulation & Sensors","description":"Introduction to digital twins, simulation environments, and sensor integration in humanoid robotics development.","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-2-digital-twins-simulation/introduction"},{"id":"modules/module-4-vision-language-action/llm-robotics-convergence","title":"LLM-Robotics Convergence","description":"Understanding how Large Language Models (LLMs) converge with robotics to enable natural language interaction with humanoid robots.","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-4-vision-language-action/llm-robotics-convergence"},{"id":"modules/module-1-ros2-nervous-system/module-1-ros2-nervous-system","title":"Module 1 - The Robotic Nervous System (ROS 2)","description":"Introduction to ROS 2 as the robotic nervous system, covering fundamental concepts, communication patterns, and humanoid robotics applications.","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-1-ros2-nervous-system/"},{"id":"modules/module-2-digital-twins-simulation/module-2-digital-twins-simulation","title":"Module 2 - Digital Twins - Simulation & Sensors","description":"Introduction to digital twins, simulation environments, and sensor integration for humanoid robotics, covering how virtual replicas enable safe testing and optimization.","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-2-digital-twins-simulation/"},{"id":"modules/module-3-ai-robot-brain/module-3-ai-robot-brain","title":"Module 3 - The AI-Robot Brain (NVIDIA Isaac\u2122)","description":"Introduction to advanced perception and training for humanoid robots using NVIDIA Isaac Sim, Isaac ROS, and Nav2 for autonomous navigation capabilities.","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-3-ai-robot-brain/"},{"id":"modules/module-4-vision-language-action/module-4-vision-language-action","title":"Module 4 - Vision-Language-Action (VLA)","description":"Introduction to Vision-Language-Action (VLA) systems, covering LLM-robotics convergence, voice-to-action, cognitive planning, and complete VLA pipeline integration.","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-4-vision-language-action/"},{"id":"modules/module-4-vision-language-action/module-integration","title":"Module Integration - Connecting VLA to Previous Modules","description":"Understanding how VLA concepts connect to and build upon concepts from Modules 1, 2, and 3, including ROS 2 integration, simulation support, and perception integration.","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-4-vision-language-action/module-integration"},{"id":"modules/module-4-vision-language-action/safety-validation","title":"Safety & Validation of LLM-Generated Plans","description":"Understanding how LLM-generated action plans are validated and executed safely, including plan verification and constraint checking.","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-4-vision-language-action/safety-validation"},{"id":"modules/module-4-vision-language-action/voice-to-action","title":"Voice-to-Action Using OpenAI Whisper","description":"Understanding how OpenAI Whisper enables voice-to-action capabilities for humanoid robots, including the complete pipeline from audio capture to action generation.","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-4-vision-language-action/voice-to-action"}],"unlisted":false}}')}}]);