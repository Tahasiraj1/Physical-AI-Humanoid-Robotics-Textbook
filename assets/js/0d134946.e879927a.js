"use strict";(globalThis.webpackChunk=globalThis.webpackChunk||[]).push([[2780],{4728:(e,n,o)=>{o.r(n),o.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>r,metadata:()=>s,toc:()=>c});const s=JSON.parse('{"id":"modules/module-1-ros2-nervous-system/humanoid-applications","title":"Humanoid Robotics Applications","description":"Connecting ROS 2 concepts to humanoid robotics applications including sensor integration, actuator coordination, and locomotion control.","source":"@site/docs/modules/module-1-ros2-nervous-system/humanoid-applications.md","sourceDirName":"modules/module-1-ros2-nervous-system","slug":"/modules/module-1-ros2-nervous-system/humanoid-applications","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-1-ros2-nervous-system/humanoid-applications","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"ros2","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/ros-2"},{"inline":true,"label":"humanoid-robotics","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/humanoid-robotics"},{"inline":true,"label":"applications","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/applications"},{"inline":true,"label":"sensors","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/sensors"},{"inline":true,"label":"actuators","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/actuators"},{"inline":true,"label":"locomotion","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/locomotion"}],"version":"current","sidebarPosition":4,"frontMatter":{"id":"humanoid-applications","title":"Humanoid Robotics Applications","sidebar_position":4,"description":"Connecting ROS 2 concepts to humanoid robotics applications including sensor integration, actuator coordination, and locomotion control.","tags":["ros2","humanoid-robotics","applications","sensors","actuators","locomotion"],"learning_objectives":["lo-003"]},"sidebar":"textbookSidebar","previous":{"title":"Communication Patterns","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-1-ros2-nervous-system/communication-patterns"},"next":{"title":"Workspace Overview","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-1-ros2-nervous-system/workspace-overview"}}');var t=o(4848),i=o(8453);const r={id:"humanoid-applications",title:"Humanoid Robotics Applications",sidebar_position:4,description:"Connecting ROS 2 concepts to humanoid robotics applications including sensor integration, actuator coordination, and locomotion control.",tags:["ros2","humanoid-robotics","applications","sensors","actuators","locomotion"],learning_objectives:["lo-003"]},a="Humanoid Robotics Applications",l={},c=[{value:"ROS 2 in Humanoid Robotics",id:"ros-2-in-humanoid-robotics",level:2},{value:"Sensor Integration with Topics",id:"sensor-integration-with-topics",level:2},{value:"Vision Sensors",id:"vision-sensors",level:3},{value:"Proprioceptive Sensors",id:"proprioceptive-sensors",level:3},{value:"Actuator Coordination with Services",id:"actuator-coordination-with-services",level:2},{value:"Coordinated Movement",id:"coordinated-movement",level:3},{value:"Configuration Services",id:"configuration-services",level:3},{value:"Locomotion Control with Actions",id:"locomotion-control-with-actions",level:2},{value:"Walking Action",id:"walking-action",level:3},{value:"Mapping ROS 2 Components to Robot Subsystems",id:"mapping-ros-2-components-to-robot-subsystems",level:2},{value:"Bridging Python Agents to ROS Controllers",id:"bridging-python-agents-to-ros-controllers",level:3},{value:"Agent-Controller Communication Patterns",id:"agent-controller-communication-patterns",level:4},{value:"Example: Agent Using Topics",id:"example-agent-using-topics",level:4},{value:"Example: Agent Using Services",id:"example-agent-using-services",level:4},{value:"Example: Agent Using Actions",id:"example-agent-using-actions",level:4},{value:"Complete System Example",id:"complete-system-example",level:2},{value:"Key Takeaways",id:"key-takeaways",level:2},{value:"Next Steps",id:"next-steps",level:2}];function d(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",table:"table",tbody:"tbody",td:"td",th:"th",thead:"thead",tr:"tr",ul:"ul",...(0,i.R)(),...e.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(n.header,{children:(0,t.jsx)(n.h1,{id:"humanoid-robotics-applications",children:"Humanoid Robotics Applications"})}),"\n",(0,t.jsx)(n.p,{children:"Now that you understand ROS 2's core concepts and communication patterns, let's explore how these apply specifically to humanoid robotics. Humanoid robots present unique challenges that make ROS 2's distributed communication architecture particularly valuable."}),"\n",(0,t.jsx)(n.h2,{id:"ros-2-in-humanoid-robotics",children:"ROS 2 in Humanoid Robotics"}),"\n",(0,t.jsx)(n.p,{children:"Humanoid robots are among the most complex robotic systems, requiring coordination between:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Multiple sensors"})," (vision, proprioception, touch, balance)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Numerous actuators"})," (joints in arms, legs, hands, head)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Complex processing"})," (perception, planning, control)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Real-time coordination"})," for stable, natural movement"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"ROS 2's distributed architecture makes it possible to manage this complexity by allowing specialized nodes to handle different aspects of the robot's operation."}),"\n",(0,t.jsx)(n.h2,{id:"sensor-integration-with-topics",children:"Sensor Integration with Topics"}),"\n",(0,t.jsx)(n.p,{children:"Humanoid robots rely on extensive sensor arrays to perceive their environment and understand their own state. ROS 2 topics are ideal for streaming this continuous sensor data."}),"\n",(0,t.jsx)(n.h3,{id:"vision-sensors",children:"Vision Sensors"}),"\n",(0,t.jsx)(n.p,{children:"Humanoid robots typically have multiple cameras (head-mounted, hand-mounted) that generate continuous image streams:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Example: Camera sensor node publishing images\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\n\nclass HeadCameraNode(Node):\n    def __init__(self):\n        super().__init__('head_camera')\n        self.publisher = self.create_publisher(Image, 'sensors/head_camera/image', 10)\n        self.timer = self.create_timer(0.033, self.publish_image)  # 30 FPS\n    \n    def publish_image(self):\n        # Capture image from head-mounted camera\n        msg = Image()\n        # ... populate with camera data ...\n        self.publisher.publish(msg)\n"})}),"\n",(0,t.jsx)(n.p,{children:"Multiple processing nodes can subscribe to these image topics:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object detection node"})," - Identifies objects in the scene"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Face recognition node"})," - Recognizes human faces"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Navigation node"})," - Uses visual information for localization"]}),"\n"]}),"\n",(0,t.jsx)(n.h3,{id:"proprioceptive-sensors",children:"Proprioceptive Sensors"}),"\n",(0,t.jsx)(n.p,{children:"Proprioceptive sensors (joint encoders, IMUs, force sensors) provide continuous feedback about the robot's body state:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Example: Joint state publisher\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import JointState\n\nclass JointStatePublisher(Node):\n    def __init__(self):\n        super().__init__('joint_state_publisher')\n        self.publisher = self.create_publisher(JointState, 'sensors/joint_states', 10)\n        self.timer = self.create_timer(0.01, self.publish_states)  # 100 Hz\n    \n    def publish_states(self):\n        msg = JointState()\n        # Populate with joint positions, velocities, efforts\n        # ... read from robot hardware ...\n        self.publisher.publish(msg)\n"})}),"\n",(0,t.jsx)(n.p,{children:"This joint state information flows to:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Control nodes"})," - For closed-loop control of each joint"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Balance node"})," - For maintaining stability"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Planning node"})," - For motion planning"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"actuator-coordination-with-services",children:"Actuator Coordination with Services"}),"\n",(0,t.jsx)(n.p,{children:"Humanoid robots have many actuators (typically 20-40+ joints) that must be coordinated precisely. Services are useful for on-demand actuator control and configuration."}),"\n",(0,t.jsx)(n.h3,{id:"coordinated-movement",children:"Coordinated Movement"}),"\n",(0,t.jsx)(n.p,{children:"When a humanoid robot needs to perform a coordinated movement (like reaching for an object), multiple joints must move together:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Example: Actuator coordination service\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import SetBool\n\nclass ArmControlService(Node):\n    def __init__(self):\n        super().__init__('arm_control')\n        self.srv = self.create_service(\n            SetBool,\n            'control/arm/move_to_pose',\n            self.move_arm_callback\n        )\n    \n    def move_arm_callback(self, request, response):\n        if request.data:  # If request is to move\n            # Coordinate multiple joints to reach target pose\n            # Shoulder, elbow, wrist joints move together\n            response.success = self.execute_coordinated_movement()\n            response.message = \"Arm movement executed\" if response.success else \"Movement failed\"\n        return response\n"})}),"\n",(0,t.jsx)(n.h3,{id:"configuration-services",children:"Configuration Services"}),"\n",(0,t.jsx)(n.p,{children:"Services are also useful for configuring actuator parameters:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Example: Setting joint limits or control parameters\nclass JointConfigService(Node):\n    def __init__(self):\n        super().__init__('joint_config')\n        self.srv = self.create_service(\n            # Custom service type for joint configuration\n            'joint_config',\n            self.config_callback\n        )\n    \n    def config_callback(self, request, response):\n        # Set joint speed limits, torque limits, etc.\n        # This is a synchronous operation - we need confirmation\n        response.success = True\n        return response\n"})}),"\n",(0,t.jsx)(n.h2,{id:"locomotion-control-with-actions",children:"Locomotion Control with Actions"}),"\n",(0,t.jsx)(n.p,{children:"Walking and other locomotion behaviors are long-running tasks that require continuous feedback and the ability to adapt or cancel. Actions are perfect for these scenarios."}),"\n",(0,t.jsx)(n.h3,{id:"walking-action",children:"Walking Action"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:"# Example: Walking action for humanoid robot\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionServer\n# Custom action type for navigation\nfrom custom_msgs.action import WalkToGoal\n\nclass WalkingActionServer(Node):\n    def __init__(self):\n        super().__init__('walking_server')\n        self._action_server = ActionServer(\n            self,\n            WalkToGoal,\n            'locomotion/walk',\n            self.walk_callback\n        )\n    \n    def walk_callback(self, goal_handle):\n        goal = goal_handle.request\n        feedback_msg = WalkToGoal.Feedback()\n        \n        # Execute walking with continuous feedback\n        total_steps = self.calculate_steps(goal.target_pose)\n        current_step = 0\n        \n        while current_step < total_steps:\n            # Execute one walking step\n            # Update balance, shift weight, lift foot, place foot\n            current_step += 1\n            \n            # Provide feedback\n            feedback_msg.current_step = current_step\n            feedback_msg.total_steps = total_steps\n            feedback_msg.current_pose = self.get_current_pose()\n            goal_handle.publish_feedback(feedback_msg)\n            \n            # Check for cancellation (e.g., obstacle detected)\n            if goal_handle.is_cancel_requested:\n                goal_handle.canceled()\n                return WalkToGoal.Result()\n        \n        # Walking complete\n        goal_handle.succeed()\n        result = WalkToGoal.Result()\n        result.final_pose = self.get_current_pose()\n        return result\n"})}),"\n",(0,t.jsx)(n.p,{children:"This walking action provides:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Continuous feedback"})," on progress (current step, current pose)"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Ability to cancel"})," if an obstacle is detected"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Final result"})," with the achieved pose"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"mapping-ros-2-components-to-robot-subsystems",children:"Mapping ROS 2 Components to Robot Subsystems"}),"\n",(0,t.jsx)(n.p,{children:"Here's how ROS 2 components map to a humanoid robot's subsystems:"}),"\n",(0,t.jsxs)(n.table,{children:[(0,t.jsx)(n.thead,{children:(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.th,{children:"Robot Subsystem"}),(0,t.jsx)(n.th,{children:"ROS 2 Component"}),(0,t.jsx)(n.th,{children:"Communication Pattern"}),(0,t.jsx)(n.th,{children:"Example"})]})}),(0,t.jsxs)(n.tbody,{children:[(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Vision System"}),(0,t.jsx)(n.td,{children:"Camera nodes"}),(0,t.jsx)(n.td,{children:"Topics"}),(0,t.jsx)(n.td,{children:"Head camera \u2192 Image processing nodes"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Proprioception"}),(0,t.jsx)(n.td,{children:"Sensor nodes"}),(0,t.jsx)(n.td,{children:"Topics"}),(0,t.jsx)(n.td,{children:"Joint encoders \u2192 State estimation"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Balance Control"}),(0,t.jsx)(n.td,{children:"Control nodes"}),(0,t.jsx)(n.td,{children:"Topics + Services"}),(0,t.jsx)(n.td,{children:"IMU data (topic) \u2192 Balance service calls"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Arm Control"}),(0,t.jsx)(n.td,{children:"Actuator nodes"}),(0,t.jsx)(n.td,{children:"Services"}),(0,t.jsx)(n.td,{children:"Reach command \u2192 Arm coordination service"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Leg Control"}),(0,t.jsx)(n.td,{children:"Locomotion nodes"}),(0,t.jsx)(n.td,{children:"Actions"}),(0,t.jsx)(n.td,{children:"Walk command \u2192 Walking action"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"Planning"}),(0,t.jsx)(n.td,{children:"Planning nodes"}),(0,t.jsx)(n.td,{children:"Topics + Services"}),(0,t.jsx)(n.td,{children:"Sensor data (topics) \u2192 Plan requests (services)"})]}),(0,t.jsxs)(n.tr,{children:[(0,t.jsx)(n.td,{children:"High-level Control"}),(0,t.jsx)(n.td,{children:"Behavior nodes"}),(0,t.jsx)(n.td,{children:"Actions"}),(0,t.jsx)(n.td,{children:'"Pick up object" \u2192 Manipulation action'})]})]})]}),"\n",(0,t.jsx)(n.h3,{id:"bridging-python-agents-to-ros-controllers",children:"Bridging Python Agents to ROS Controllers"}),"\n",(0,t.jsxs)(n.p,{children:["High-level Python agents (AI systems, planning algorithms, decision-making modules) often need to communicate with low-level ROS 2 controllers that manage robot hardware. ",(0,t.jsx)(n.strong,{children:"Agent bridging"})," refers to the patterns and mechanisms that connect these high-level systems to ROS 2-based robot controllers using ",(0,t.jsx)(n.code,{children:"rclpy"})," (ROS 2 Python client library)."]}),"\n",(0,t.jsx)(n.h4,{id:"agent-controller-communication-patterns",children:"Agent-Controller Communication Patterns"}),"\n",(0,t.jsx)(n.p,{children:"Python agents can bridge to ROS 2 controllers using the same communication patterns available in ROS 2:"}),"\n",(0,t.jsxs)(n.ul,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Topics"}),": For streaming commands or receiving continuous state updates"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Services"}),": For on-demand requests and synchronous operations"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Actions"}),": For long-running tasks that require feedback and cancellation"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"Each pattern serves different needs in agent-controller communication, allowing agents to choose the most appropriate mechanism for their specific interaction."}),"\n",(0,t.jsx)(n.h4,{id:"example-agent-using-topics",children:"Example: Agent Using Topics"}),"\n",(0,t.jsx)(n.p,{children:"Agents can publish commands to topics that controllers subscribe to:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Example: Python agent publishing movement commands to ROS 2 controller\nimport rclpy\nfrom rclpy.node import Node\nfrom geometry_msgs.msg import Twist\n\nclass AgentNode(Node):\n    """High-level agent that bridges to ROS 2 controllers via topics"""\n    \n    def __init__(self):\n        super().__init__(\'agent_node\')\n        # Agent publishes commands to controller\n        self.command_publisher = self.create_publisher(\n            Twist,\n            \'/robot/cmd_vel\',  # Controller subscribes to this topic\n            10\n        )\n        # Agent subscribes to robot state from controller\n        self.state_subscription = self.create_subscription(\n            JointState,\n            \'/joint_states\',  # Controller publishes state here\n            self.state_callback,\n            10\n        )\n        self.current_state = None\n    \n    def state_callback(self, msg):\n        """Receive robot state from controller"""\n        self.current_state = msg\n        # Agent can make decisions based on current state\n    \n    def send_movement_command(self, linear_x, angular_z):\n        """Agent sends movement command to controller"""\n        msg = Twist()\n        msg.linear.x = linear_x\n        msg.angular.z = angular_z\n        self.command_publisher.publish(msg)\n        self.get_logger().info(f\'Agent sent command: linear={linear_x}, angular={angular_z}\')\n'})}),"\n",(0,t.jsx)(n.p,{children:"This pattern enables agents to send continuous commands (like velocity commands) while receiving state updates, creating a feedback loop for control."}),"\n",(0,t.jsx)(n.h4,{id:"example-agent-using-services",children:"Example: Agent Using Services"}),"\n",(0,t.jsx)(n.p,{children:"Agents can call services provided by controllers for on-demand operations:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Example: Python agent calling controller service\nimport rclpy\nfrom rclpy.node import Node\nfrom example_interfaces.srv import Trigger\n\nclass AgentNode(Node):\n    """High-level agent bridging to ROS 2 controller via services"""\n    \n    def __init__(self):\n        super().__init__(\'agent_node\')\n        # Agent creates service client to call controller service\n        self.arm_control_client = self.create_client(\n            Trigger,\n            \'/robot/arm/activate\'  # Controller provides this service\n        )\n    \n    def activate_arm(self):\n        """Agent requests controller to activate arm"""\n        request = Trigger.Request()\n        future = self.arm_control_client.call_async(request)\n        rclpy.spin_until_future_complete(self, future)\n        \n        if future.result().success:\n            self.get_logger().info(\'Agent successfully activated arm via controller\')\n            return True\n        else:\n            self.get_logger().warn(\'Agent failed to activate arm\')\n            return False\n'})}),"\n",(0,t.jsx)(n.p,{children:"This pattern is useful when agents need to request specific actions from controllers and wait for confirmation before proceeding."}),"\n",(0,t.jsx)(n.h4,{id:"example-agent-using-actions",children:"Example: Agent Using Actions"}),"\n",(0,t.jsx)(n.p,{children:"Agents can use actions for complex behaviors that require feedback:"}),"\n",(0,t.jsx)(n.pre,{children:(0,t.jsx)(n.code,{className:"language-python",children:'# Example: Python agent using action to control robot behavior\nimport rclpy\nfrom rclpy.node import Node\nfrom rclpy.action import ActionClient\nfrom nav2_msgs.action import NavigateToPose\n\nclass AgentNode(Node):\n    """High-level agent bridging to ROS 2 controller via actions"""\n    \n    def __init__(self):\n        super().__init__(\'agent_node\')\n        # Agent creates action client\n        self._action_client = ActionClient(\n            self,\n            NavigateToPose,\n            \'/robot/navigate_to_pose\'  # Controller provides this action\n        )\n    \n    def navigate_to_goal(self, target_x, target_y):\n        """Agent sends navigation goal to controller via action"""\n        goal_msg = NavigateToPose.Goal()\n        goal_msg.pose.pose.position.x = target_x\n        goal_msg.pose.pose.position.y = target_y\n        \n        self._action_client.wait_for_server()\n        send_goal_future = self._action_client.send_goal_async(\n            goal_msg,\n            feedback_callback=self.feedback_callback\n        )\n        rclpy.spin_until_future_complete(self, send_goal_future)\n        \n        goal_handle = send_goal_future.result()\n        if not goal_handle.accepted:\n            self.get_logger().error(\'Agent goal rejected by controller\')\n            return\n        \n        # Wait for result\n        result_future = goal_handle.get_result_async()\n        rclpy.spin_until_future_complete(self, result_future)\n        \n        result = result_future.result().result\n        self.get_logger().info(f\'Agent navigation completed: {result}\')\n    \n    def feedback_callback(self, feedback_msg):\n        """Agent receives feedback from controller during action execution"""\n        self.get_logger().info(\n            f\'Agent received feedback: distance remaining = {feedback_msg.distance_remaining}\'\n        )\n'})}),"\n",(0,t.jsx)(n.p,{children:"This pattern enables agents to execute complex behaviors (like navigation) while receiving progress updates and maintaining the ability to cancel if needed."}),"\n",(0,t.jsxs)(n.p,{children:["These bridging patterns allow Python agents to leverage ROS 2's communication infrastructure while maintaining separation between high-level decision-making (agents) and low-level control (controllers). For more details on these communication patterns, see ",(0,t.jsx)(n.a,{href:"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-1-ros2-nervous-system/communication-patterns",children:"Communication Patterns"}),"."]}),"\n",(0,t.jsx)(n.h2,{id:"complete-system-example",children:"Complete System Example"}),"\n",(0,t.jsx)(n.p,{children:"In a humanoid robot picking up an object:"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Vision nodes"})," publish camera images to topics"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Object detection node"})," subscribes to images, publishes object location to a topic"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Planning node"})," subscribes to object location, calls arm control service to plan trajectory"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Arm control service"})," coordinates multiple joints synchronously"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Grasping action"})," executes the grasp with feedback on finger positions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Balance node"})," continuously subscribes to IMU data (topic) to maintain stability"]}),"\n"]}),"\n",(0,t.jsx)(n.p,{children:"This demonstrates how all three communication patterns work together in a real humanoid robot application."}),"\n",(0,t.jsx)(n.h2,{id:"key-takeaways",children:"Key Takeaways"}),"\n",(0,t.jsxs)(n.ol,{children:["\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Topics"})," stream continuous sensor data to multiple processing nodes"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Services"})," coordinate actuators for synchronous, on-demand actions"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Actions"})," execute complex behaviors like walking with feedback and cancellation"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"Humanoid robots"})," use all three patterns simultaneously for different subsystems"]}),"\n",(0,t.jsxs)(n.li,{children:[(0,t.jsx)(n.strong,{children:"ROS 2's distributed architecture"})," makes it possible to manage the complexity of humanoid systems"]}),"\n"]}),"\n",(0,t.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,t.jsxs)(n.p,{children:["Now that you understand how ROS 2 applies to humanoid robotics, proceed to ",(0,t.jsx)(n.a,{href:"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-1-ros2-nervous-system/workspace-overview",children:"Workspace Overview"})," to learn about ROS 2 workspace structure, or review the ",(0,t.jsx)(n.a,{href:"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-1-ros2-nervous-system/glossary",children:"Glossary"})," for quick reference to key terms."]})]})}function m(e={}){const{wrapper:n}={...(0,i.R)(),...e.components};return n?(0,t.jsx)(n,{...e,children:(0,t.jsx)(d,{...e})}):d(e)}},8453:(e,n,o)=>{o.d(n,{R:()=>r,x:()=>a});var s=o(6540);const t={},i=s.createContext(t);function r(e){const n=s.useContext(i);return s.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(t):e.components||t:r(e.components),s.createElement(i.Provider,{value:n},e.children)}}}]);