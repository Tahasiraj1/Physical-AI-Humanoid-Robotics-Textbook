"use strict";(globalThis.webpackChunk=globalThis.webpackChunk||[]).push([[2801],{1293:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>h,frontMatter:()=>r,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"modules/module-2-digital-twins-simulation/sensor-integration","title":"Sensor Integration","description":"Sensor types and ROS 2 integration for humanoid robots, covering vision, proprioception, and tactile sensors.","source":"@site/docs/modules/module-2-digital-twins-simulation/sensor-integration.md","sourceDirName":"modules/module-2-digital-twins-simulation","slug":"/modules/module-2-digital-twins-simulation/sensor-integration","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-2-digital-twins-simulation/sensor-integration","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"sensors","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/sensors"},{"inline":true,"label":"ros2-integration","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/ros-2-integration"},{"inline":true,"label":"sensor-data-flow","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/sensor-data-flow"},{"inline":true,"label":"vision","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/vision"},{"inline":true,"label":"proprioception","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/proprioception"},{"inline":true,"label":"tactile","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/tags/tactile"}],"version":"current","sidebarPosition":4,"frontMatter":{"id":"sensor-integration","title":"Sensor Integration","sidebar_position":4,"description":"Sensor types and ROS 2 integration for humanoid robots, covering vision, proprioception, and tactile sensors.","tags":["sensors","ros2-integration","sensor-data-flow","vision","proprioception","tactile"],"learning_objectives":["lo-006"],"topic_category":"sensor"},"sidebar":"textbookSidebar","previous":{"title":"Simulation Fundamentals","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-2-digital-twins-simulation/simulation-fundamentals"},"next":{"title":"Humanoid Applications","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-2-digital-twins-simulation/humanoid-applications"}}');var o=s(4848),t=s(8453);const r={id:"sensor-integration",title:"Sensor Integration",sidebar_position:4,description:"Sensor types and ROS 2 integration for humanoid robots, covering vision, proprioception, and tactile sensors.",tags:["sensors","ros2-integration","sensor-data-flow","vision","proprioception","tactile"],learning_objectives:["lo-006"],topic_category:"sensor"},a="Sensor Integration",l={},c=[{value:"Sensor Types in Humanoid Robots",id:"sensor-types-in-humanoid-robots",level:2},{value:"Vision Sensors (Cameras)",id:"vision-sensors-cameras",level:3},{value:"How Vision Sensors Work",id:"how-vision-sensors-work",level:4},{value:"Humanoid Robotics Use Cases",id:"humanoid-robotics-use-cases",level:4},{value:"Example: Camera Data Processing",id:"example-camera-data-processing",level:4},{value:"Proprioceptive Sensors (IMUs, Joint Encoders)",id:"proprioceptive-sensors-imus-joint-encoders",level:3},{value:"Inertial Measurement Units (IMUs)",id:"inertial-measurement-units-imus",level:4},{value:"Joint Encoders",id:"joint-encoders",level:4},{value:"Humanoid Robotics Use Cases",id:"humanoid-robotics-use-cases-1",level:4},{value:"Tactile Sensors",id:"tactile-sensors",level:3},{value:"How Tactile Sensors Work",id:"how-tactile-sensors-work",level:4},{value:"Humanoid Robotics Use Cases",id:"humanoid-robotics-use-cases-2",level:4},{value:"Sensor Data Flow Through ROS 2",id:"sensor-data-flow-through-ros-2",level:2},{value:"Sensor Data Flow Architecture",id:"sensor-data-flow-architecture",level:3},{value:"How Sensors Publish Data to Topics",id:"how-sensors-publish-data-to-topics",level:3},{value:"Example: Sensor Data Publishing",id:"example-sensor-data-publishing",level:3},{value:"How Sensors Enable Robot Perception and Decision-Making",id:"how-sensors-enable-robot-perception-and-decision-making",level:2},{value:"Perception Pipeline",id:"perception-pipeline",level:3},{value:"Example: Multi-Sensor Perception",id:"example-multi-sensor-perception",level:3},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Related Content",id:"related-content",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,t.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"sensor-integration",children:"Sensor Integration"})}),"\n",(0,o.jsx)(n.p,{children:"Sensors are the primary interface between humanoid robots and their environment. They provide the data that enables robots to perceive the world, understand their own state, and make decisions. Understanding how sensors integrate with ROS 2 is essential for building functional humanoid robots."}),"\n",(0,o.jsx)(n.h2,{id:"sensor-types-in-humanoid-robots",children:"Sensor Types in Humanoid Robots"}),"\n",(0,o.jsx)(n.p,{children:"Humanoid robots use multiple sensor types to gather information about their environment and internal state. Each sensor type provides different information that contributes to robot perception and decision-making."}),"\n",(0,o.jsx)(n.h3,{id:"vision-sensors-cameras",children:"Vision Sensors (Cameras)"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Vision sensors"})," capture visual information from the environment, enabling robots to see and understand their surroundings."]}),"\n",(0,o.jsx)(n.h4,{id:"how-vision-sensors-work",children:"How Vision Sensors Work"}),"\n",(0,o.jsx)(n.p,{children:"Cameras capture light and convert it into digital images:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Image capture"}),": Light enters through lens, hits image sensor"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Digital conversion"}),": Sensor converts light to pixel values"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Image processing"}),": Raw data processed into usable format"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Data publishing"}),": Images published to ROS 2 topics"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"humanoid-robotics-use-cases",children:"Humanoid Robotics Use Cases"}),"\n",(0,o.jsx)(n.p,{children:"Vision sensors enable humanoid robots to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigate"})," by recognizing landmarks and obstacles"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulate objects"})," by identifying and locating items"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Interact with humans"})," by recognizing faces and gestures"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Understand scenes"})," by analyzing spatial relationships"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"example-camera-data-processing",children:"Example: Camera Data Processing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Example: Processing camera data in humanoid robotics\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\n\nclass VisionProcessingNode(Node):\n    """ROS 2 node for processing camera data in humanoid robot"""\n    \n    def __init__(self):\n        super().__init__(\'vision_processing_node\')\n        \n        # Subscribe to camera topic\n        self.subscription = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',  # Camera topic\n            self.image_callback,\n            10\n        )\n        \n        # Publish processed results\n        self.publisher = self.create_publisher(\n            Image,\n            \'/vision/processed_image\',\n            10\n        )\n        \n        self.bridge = CvBridge()\n        \n    def image_callback(self, msg):\n        """Process incoming camera image"""\n        # Convert ROS image message to OpenCV format\n        cv_image = self.bridge.imgmsg_to_cv2(msg, \'bgr8\')\n        \n        # Process image (e.g., object detection, edge detection)\n        processed_image = self.process_image(cv_image)\n        \n        # Convert back to ROS message\n        ros_image = self.bridge.cv2_to_imgmsg(processed_image, \'bgr8\')\n        \n        # Publish processed image\n        self.publisher.publish(ros_image)\n        \n    def process_image(self, image):\n        """Process image for humanoid robot perception"""\n        # Example: Edge detection for navigation\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        edges = cv2.Canny(gray, 50, 150)\n        return edges\n'})}),"\n",(0,o.jsx)(n.p,{children:"This example shows how vision sensors integrate with ROS 2:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Subscribing"})," to camera topic (",(0,o.jsx)(n.code,{children:"/camera/image_raw"}),")"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Processing"})," image data for perception"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Publishing"})," results to other nodes"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"proprioceptive-sensors-imus-joint-encoders",children:"Proprioceptive Sensors (IMUs, Joint Encoders)"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Proprioceptive sensors"})," measure the robot's internal state, providing information about body position, orientation, and movement."]}),"\n",(0,o.jsx)(n.h4,{id:"inertial-measurement-units-imus",children:"Inertial Measurement Units (IMUs)"}),"\n",(0,o.jsx)(n.p,{children:"IMUs measure orientation and acceleration:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Gyroscopes"}),": Measure angular velocity (rotation rate)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Accelerometers"}),": Measure linear acceleration"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Magnetometers"}),": Measure magnetic field (for heading)"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"IMUs provide critical information for:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Balance control"}),": Understanding robot orientation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Motion planning"}),": Tracking body movement"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Fall detection"}),": Detecting when robot is falling"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"joint-encoders",children:"Joint Encoders"}),"\n",(0,o.jsx)(n.p,{children:"Joint encoders measure joint angles:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Position feedback"}),": Current angle of each joint"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Velocity feedback"}),": Rate of joint movement"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Torque feedback"}),": Force applied at joints"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Joint encoders enable:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Precise control"}),": Knowing exact joint positions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Coordination"}),": Synchronizing multiple joints"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safety"}),": Detecting joint limits and collisions"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"humanoid-robotics-use-cases-1",children:"Humanoid Robotics Use Cases"}),"\n",(0,o.jsx)(n.p,{children:"Proprioceptive sensors enable humanoid robots to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Maintain balance"})," by sensing body orientation and joint positions"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Coordinate movement"})," by tracking all joint states"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Detect falls"})," by monitoring acceleration and orientation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Plan motions"})," by understanding current body configuration"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"tactile-sensors",children:"Tactile Sensors"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Tactile sensors"})," detect contact and force, enabling robots to sense touch and interaction with objects."]}),"\n",(0,o.jsx)(n.h4,{id:"how-tactile-sensors-work",children:"How Tactile Sensors Work"}),"\n",(0,o.jsx)(n.p,{children:"Tactile sensors measure:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Contact detection"}),": Whether robot is touching something"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Force magnitude"}),": How hard the contact is"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Force direction"}),": Direction of applied force"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Contact location"}),": Where on the robot contact occurs"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"humanoid-robotics-use-cases-2",children:"Humanoid Robotics Use Cases"}),"\n",(0,o.jsx)(n.p,{children:"Tactile sensors enable humanoid robots to:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Grasp objects"})," by sensing contact and adjusting grip force"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Detect collisions"})," by sensing unexpected contact"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Interact safely"})," with humans by sensing touch"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Manipulate objects"})," by feeling contact forces"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"sensor-data-flow-through-ros-2",children:"Sensor Data Flow Through ROS 2"}),"\n",(0,o.jsxs)(n.p,{children:["Sensor data flows through ROS 2 using the ",(0,o.jsx)(n.strong,{children:"publish-subscribe pattern"}),", as discussed in ",(0,o.jsx)(n.a,{href:"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-1-ros2-nervous-system/communication-patterns#publish-subscribe-pattern",children:"Module 1's communication patterns"}),". This enables decoupled, asynchronous communication between sensors and processing nodes."]}),"\n",(0,o.jsx)(n.h3,{id:"sensor-data-flow-architecture",children:"Sensor Data Flow Architecture"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:"graph LR\n    A[Vision Sensor] --\x3e|Publish| B[/camera/image_raw]\n    C[IMU Sensor] --\x3e|Publish| D[/imu/data]\n    E[Joint Encoder] --\x3e|Publish| F[/joint_states]\n    G[Tactile Sensor] --\x3e|Publish| H[/tactile/contact]\n    \n    B --\x3e I[Vision Processing Node]\n    D --\x3e J[Balance Control Node]\n    F --\x3e K[Motion Planning Node]\n    H --\x3e L[Grasp Control Node]\n    \n    I --\x3e M[Perception System]\n    J --\x3e M\n    K --\x3e M\n    L --\x3e M\n    M --\x3e N[Decision Making]\n    \n    style A fill:#e1f5ff\n    style C fill:#fff4e1\n    style E fill:#e8f5e9\n    style G fill:#fce4ec\n    style M fill:#f3e5f5\n    style N fill:#e0f2f1\n"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Figure 1: Sensor data flow through ROS 2 topics, showing how different sensor types publish data that processing nodes consume for perception and decision-making."})}),"\n",(0,o.jsx)(n.h3,{id:"how-sensors-publish-data-to-topics",children:"How Sensors Publish Data to Topics"}),"\n",(0,o.jsx)(n.p,{children:"Sensors publish data to ROS 2 topics using the following pattern:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sensor node"})," captures data from hardware"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Data formatting"})," converts sensor data to ROS 2 message format"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Topic publishing"})," sends message to topic"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Subscriber nodes"})," receive and process data"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"example-sensor-data-publishing",children:"Example: Sensor Data Publishing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Example: Publishing sensor data to ROS 2 topics\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu, JointState\nfrom geometry_msgs.msg import Vector3\nimport math\n\nclass SensorNode(Node):\n    \"\"\"ROS 2 node for publishing sensor data from humanoid robot\"\"\"\n    \n    def __init__(self):\n        super().__init__('sensor_node')\n        \n        # Create publishers for different sensor types\n        self.imu_publisher = self.create_publisher(\n            Imu,\n            '/imu/data',\n            10\n        )\n        \n        self.joint_publisher = self.create_publisher(\n            JointState,\n            '/joint_states',\n            10\n        )\n        \n        # Timer to publish sensor data at regular intervals\n        self.timer = self.create_timer(0.01, self.publish_sensor_data)  # 100 Hz\n        \n    def publish_sensor_data(self):\n        \"\"\"Publish sensor data from humanoid robot\"\"\"\n        # Publish IMU data\n        imu_msg = Imu()\n        imu_msg.linear_acceleration.x = self.read_accelerometer_x()\n        imu_msg.linear_acceleration.y = self.read_accelerometer_y()\n        imu_msg.linear_acceleration.z = self.read_accelerometer_z()\n        imu_msg.angular_velocity.x = self.read_gyroscope_x()\n        imu_msg.angular_velocity.y = self.read_gyroscope_y()\n        imu_msg.angular_velocity.z = self.read_gyroscope_z()\n        self.imu_publisher.publish(imu_msg)\n        \n        # Publish joint encoder data\n        joint_msg = JointState()\n        joint_msg.name = ['shoulder', 'elbow', 'hip', 'knee', 'ankle']\n        joint_msg.position = [\n            self.read_joint_angle('shoulder'),\n            self.read_joint_angle('elbow'),\n            self.read_joint_angle('hip'),\n            self.read_joint_angle('knee'),\n            self.read_joint_angle('ankle')\n        ]\n        joint_msg.velocity = [0.0] * 5  # Joint velocities\n        joint_msg.effort = [0.0] * 5    # Joint torques\n        self.joint_publisher.publish(joint_msg)\n        \n    def read_accelerometer_x(self):\n        \"\"\"Read accelerometer X value (example)\"\"\"\n        # In real implementation, this would read from hardware\n        return 0.0\n        \n    def read_joint_angle(self, joint_name):\n        \"\"\"Read joint angle from encoder (example)\"\"\"\n        # In real implementation, this would read from hardware\n        return 0.0\n"})}),"\n",(0,o.jsx)(n.p,{children:"This example demonstrates:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Creating publishers"})," for different sensor types"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Formatting data"})," as ROS 2 messages"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Publishing at regular intervals"})," (100 Hz for proprioceptive sensors)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multiple sensor types"})," publishing to different topics"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"how-sensors-enable-robot-perception-and-decision-making",children:"How Sensors Enable Robot Perception and Decision-Making"}),"\n",(0,o.jsx)(n.p,{children:"Sensors provide the raw data that enables robots to:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perceive the environment"})," through vision and tactile sensors"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Understand internal state"})," through proprioceptive sensors"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Make decisions"})," based on combined sensor information"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Execute actions"})," informed by sensor feedback"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"perception-pipeline",children:"Perception Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"The perception pipeline processes sensor data:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Raw sensor data"})," \u2192 Individual sensor readings"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sensor fusion"})," \u2192 Combining data from multiple sensors"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Feature extraction"})," \u2192 Identifying relevant information"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"State estimation"})," \u2192 Understanding current situation"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Decision making"})," \u2192 Choosing appropriate actions"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"example-multi-sensor-perception",children:"Example: Multi-Sensor Perception"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Example: Using multiple sensors for perception and decision-making\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu, JointState\n\nclass PerceptionNode(Node):\n    """ROS 2 node that fuses sensor data for perception"""\n    \n    def __init__(self):\n        super().__init__(\'perception_node\')\n        \n        # Subscribe to multiple sensor topics\n        self.create_subscription(Image, \'/camera/image_raw\', self.vision_callback, 10)\n        self.create_subscription(Imu, \'/imu/data\', self.imu_callback, 10)\n        self.create_subscription(JointState, \'/joint_states\', self.joint_callback, 10)\n        \n        # Store latest sensor data\n        self.latest_image = None\n        self.latest_imu = None\n        self.latest_joints = None\n        \n    def vision_callback(self, msg):\n        """Process vision data"""\n        self.latest_image = msg\n        self.update_perception()\n        \n    def imu_callback(self, msg):\n        """Process IMU data"""\n        self.latest_imu = msg\n        self.update_perception()\n        \n    def joint_callback(self, msg):\n        """Process joint encoder data"""\n        self.latest_joints = msg\n        self.update_perception()\n        \n    def update_perception(self):\n        """Fuse sensor data for perception and decision-making"""\n        if not all([self.latest_image, self.latest_imu, self.latest_joints]):\n            return\n            \n        # Fuse sensor data\n        robot_state = self.estimate_robot_state()\n        environment_state = self.analyze_environment()\n        \n        # Make decision based on perception\n        action = self.decide_action(robot_state, environment_state)\n        self.execute_action(action)\n        \n    def estimate_robot_state(self):\n        """Estimate robot state from proprioceptive sensors"""\n        # Use IMU for orientation\n        orientation = self.latest_imu.orientation\n        \n        # Use joint encoders for body configuration\n        joint_positions = self.latest_joints.position\n        \n        return {\n            \'orientation\': orientation,\n            \'joint_positions\': joint_positions\n        }\n        \n    def analyze_environment(self):\n        """Analyze environment from vision sensor"""\n        # Process image to understand environment\n        # (object detection, obstacle identification, etc.)\n        return {\'obstacles\': [], \'objects\': []}\n        \n    def decide_action(self, robot_state, environment_state):\n        """Make decision based on perception"""\n        # Decision-making logic using sensor data\n        return \'move_forward\'\n        \n    def execute_action(self, action):\n        """Execute decided action"""\n        self.get_logger().info(f\'Executing action: {action}\')\n'})}),"\n",(0,o.jsx)(n.p,{children:"This example shows how:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multiple sensors"})," provide different types of information"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sensor fusion"})," combines data for comprehensive understanding"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception"})," extracts meaningful information from raw data"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Decision-making"})," uses perception to choose actions"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"Humanoid robots use multiple sensor types: vision sensors (cameras) for visual perception, proprioceptive sensors (IMUs, joint encoders) for internal state, and tactile sensors for contact detection. Sensor data flows through ROS 2 topics using the publish-subscribe pattern, enabling decoupled communication between sensors and processing nodes. Sensors enable robot perception by providing raw data that is fused, processed, and used for decision-making. Understanding sensor integration with ROS 2 is essential for building functional humanoid robots."}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(n.p,{children:["Now that you understand sensor integration, proceed to ",(0,o.jsx)(n.a,{href:"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-2-digital-twins-simulation/humanoid-applications",children:"Humanoid Applications"})," to learn how digital twins are applied in practice for humanoid robotics development."]}),"\n",(0,o.jsx)(n.h2,{id:"related-content",children:"Related Content"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.a,{href:"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-1-ros2-nervous-system/communication-patterns#publish-subscribe-pattern",children:"Module 1: ROS 2 Topics"})})," - Detailed explanation of publish-subscribe pattern used for sensor data"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.a,{href:"/Physical-AI-Humanoid-Robotics-Textbook/modules/module-1-ros2-nervous-system/ros2-fundamentals#nodes",children:"Module 1: ROS 2 Nodes"})})," - Understanding how sensor nodes work"]}),"\n"]})]})}function h(e={}){const{wrapper:n}={...(0,t.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>r,x:()=>a});var i=s(6540);const o={},t=i.createContext(o);function r(e){const n=i.useContext(t);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),i.createElement(t.Provider,{value:n},e.children)}}}]);