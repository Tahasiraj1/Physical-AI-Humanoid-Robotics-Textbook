"use strict";(globalThis.webpackChunk=globalThis.webpackChunk||[]).push([[386],{1203:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>c,contentTitle:()=>l,default:()=>d,frontMatter:()=>s,metadata:()=>t,toc:()=>r});const t=JSON.parse('{"id":"modules/module-4-vision-language-action/capstone-project","title":"Capstone Project - The Autonomous Humanoid","description":"Complete VLA pipeline demonstration \u062f\u06a9\u06be\u0627\u062a\u06d2 \u06c1\u0648\u0626\u06d2 \u06a9\u06c1 simulated humanoid robot \u06a9\u06cc\u0633\u06d2 voice commands receive \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c paths plan \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c obstacles navigate \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c objects identify \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c \u0627\u0648\u0631 \u0627\u0646\u06c1\u06cc\u06ba manipulate \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u06d4","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/modules/module-4-vision-language-action/capstone-project.md","sourceDirName":"modules/module-4-vision-language-action","slug":"/modules/module-4-vision-language-action/capstone-project","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-4-vision-language-action/capstone-project","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"vla","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/vla"},{"inline":true,"label":"capstone","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/capstone"},{"inline":true,"label":"autonomous-robot","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/autonomous-robot"},{"inline":true,"label":"voice-to-action","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/voice-to-action"},{"inline":true,"label":"cognitive-planning","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/cognitive-planning"},{"inline":true,"label":"navigation","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/navigation"},{"inline":true,"label":"manipulation","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/manipulation"},{"inline":true,"label":"humanoid-robotics","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/humanoid-robotics"}],"version":"current","sidebarPosition":6,"frontMatter":{"id":"capstone-project","title":"Capstone Project - The Autonomous Humanoid","sidebar_position":6,"description":"Complete VLA pipeline demonstration \u062f\u06a9\u06be\u0627\u062a\u06d2 \u06c1\u0648\u0626\u06d2 \u06a9\u06c1 simulated humanoid robot \u06a9\u06cc\u0633\u06d2 voice commands receive \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c paths plan \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c obstacles navigate \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c objects identify \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c \u0627\u0648\u0631 \u0627\u0646\u06c1\u06cc\u06ba manipulate \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u06d4","tags":["vla","capstone","autonomous-robot","voice-to-action","cognitive-planning","navigation","manipulation","humanoid-robotics"],"learning_objectives":["lo-013"]},"sidebar":"textbookSidebar","previous":{"title":"LLM-Generated Plans \u06a9\u06cc Safety & Validation","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-4-vision-language-action/safety-validation"},"next":{"title":"Module Integration - VLA \u06a9\u0648 Previous Modules \u0633\u06d2 Connect \u06a9\u0631\u0646\u0627","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-4-vision-language-action/module-integration"}}');var o=i(4848),a=i(8453);const s={id:"capstone-project",title:"Capstone Project - The Autonomous Humanoid",sidebar_position:6,description:"Complete VLA pipeline demonstration \u062f\u06a9\u06be\u0627\u062a\u06d2 \u06c1\u0648\u0626\u06d2 \u06a9\u06c1 simulated humanoid robot \u06a9\u06cc\u0633\u06d2 voice commands receive \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c paths plan \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c obstacles navigate \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c objects identify \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c \u0627\u0648\u0631 \u0627\u0646\u06c1\u06cc\u06ba manipulate \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u06d4",tags:["vla","capstone","autonomous-robot","voice-to-action","cognitive-planning","navigation","manipulation","humanoid-robotics"],learning_objectives:["lo-013"]},l="Capstone Project: The Autonomous Humanoid",c={},r=[{value:"Project Overview",id:"project-overview",level:2},{value:"Complete VLA Pipeline",id:"complete-vla-pipeline",level:2},{value:"Step-by-Step Flow",id:"step-by-step-flow",level:2},{value:"Step 1: Voice Command",id:"step-1-voice-command",level:3},{value:"Step 2: Cognitive Planning",id:"step-2-cognitive-planning",level:3},{value:"Step 3: Path Planning",id:"step-3-path-planning",level:3},{value:"Step 4: Obstacle Navigation",id:"step-4-obstacle-navigation",level:3},{value:"Step 5: Object Identification",id:"step-5-object-identification",level:3},{value:"Step 6: Object Manipulation",id:"step-6-object-manipulation",level:3},{value:"VLA Components \u06a9\u06cc Integration",id:"vla-components-\u06a9\u06cc-integration",level:2},{value:"Voice Input Integration",id:"voice-input-integration",level:3},{value:"Cognitive Planning Integration",id:"cognitive-planning-integration",level:3},{value:"ROS 2 Action Generation Integration",id:"ros-2-action-generation-integration",level:3},{value:"Previous Modules \u0633\u06d2 Cross-References",id:"previous-modules-\u0633\u06d2-cross-references",level:2},{value:"\u0645\u0627\u0688\u06cc\u0648\u0644 2: Simulation",id:"\u0645\u0627\u0688\u06cc\u0648\u0644-2-simulation",level:3},{value:"\u0645\u0627\u0688\u06cc\u0648\u0644 3: Perception \u0627\u0648\u0631 Navigation",id:"\u0645\u0627\u0688\u06cc\u0648\u0644-3-perception-\u0627\u0648\u0631-navigation",level:3},{value:"Complete Integration Flow Diagram",id:"complete-integration-flow-diagram",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function p(n){const e={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,a.R)(),...n.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(e.header,{children:(0,o.jsx)(e.h1,{id:"capstone-project-the-autonomous-humanoid",children:"Capstone Project: The Autonomous Humanoid"})}),"\n",(0,o.jsx)(e.p,{children:"\u06cc\u06c1 capstone project complete Vision-Language-Action (VLA) pipeline demonstrate \u06a9\u0631\u062a\u0627 \u06c1\u06d2 action \u0645\u06cc\u06ba\u06d4 \u0622\u067e \u062f\u06cc\u06a9\u06be\u06cc\u06ba \u06af\u06d2 \u06a9\u06c1 simulated humanoid robot \u06a9\u06cc\u0633\u06d2 voice command receive \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c cognitive planning \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u0627 \u06c1\u06d2 path generate \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2\u060c perception \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2 obstacles navigate \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c computer vision \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2 objects identify \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c \u0627\u0648\u0631 \u0627\u0646\u06c1\u06cc\u06ba manipulate \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u2014all integrated cohesive autonomous behavior \u0645\u06cc\u06ba\u06d4"}),"\n",(0,o.jsx)(e.h2,{id:"project-overview",children:"Project Overview"}),"\n",(0,o.jsx)(e.p,{children:"Capstone project \u062a\u0645\u0627\u0645 VLA concepts integrate \u06a9\u0631\u062a\u0627 \u06c1\u06d2 single demonstration \u0645\u06cc\u06ba:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Voice input"}),": Spoken command autonomous behavior initiate \u06a9\u0631\u062a\u06cc \u06c1\u06d2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Cognitive planning"}),": Natural language action plan \u0645\u06cc\u06ba translate \u06c1\u0648\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Path planning"}),": Goal reach \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 navigation path generate \u06c1\u0648\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Obstacle navigation"}),": Robot perception \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2 obstacles around navigate \u06a9\u0631\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Object identification"}),": Computer vision target objects identify \u06a9\u0631\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Object manipulation"}),": Robot objects grasp \u0627\u0648\u0631 manipulate \u06a9\u0631\u062a\u0627 \u06c1\u06d2"]}),"\n"]}),"\n",(0,o.jsx)(e.p,{children:"\u06cc\u06c1 project \u062f\u06a9\u06be\u0627\u062a\u0627 \u06c1\u06d2 \u06a9\u06c1 voice-to-action\u060c cognitive planning\u060c perception\u060c navigation\u060c \u0627\u0648\u0631 manipulation \u06a9\u06cc\u0633\u06d2 \u0645\u0644 \u06a9\u0631 \u06a9\u0627\u0645 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba complete VLA system \u0645\u06cc\u06ba\u06d4"}),"\n",(0,o.jsx)(e.h2,{id:"complete-vla-pipeline",children:"Complete VLA Pipeline"}),"\n",(0,o.jsx)(e.p,{children:"Capstone end-to-end flow demonstrate \u06a9\u0631\u062a\u0627 \u06c1\u06d2:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{children:"Voice Command \u2192 Speech Recognition \u2192 Cognitive Planning \u2192 \nPath Planning \u2192 Obstacle Navigation \u2192 Object Identification \u2192 \nObject Manipulation \u2192 Task Completion\n"})}),"\n",(0,o.jsx)(e.p,{children:"\u06c1\u0631 stage previous \u067e\u0631 build \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c seamless autonomous behavior \u0628\u0646\u0627\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4"}),"\n",(0,o.jsx)(e.h2,{id:"step-by-step-flow",children:"Step-by-Step Flow"}),"\n",(0,o.jsx)(e.h3,{id:"step-1-voice-command",children:"Step 1: Voice Command"}),"\n",(0,o.jsxs)(e.p,{children:["User command \u0628\u0648\u0644\u062a\u0627 \u06c1\u06d2: ",(0,o.jsx)(e.strong,{children:'"table \u0633\u06d2 \u0633\u0631\u062e \u06a9\u067e \u0627\u0679\u06be\u0627\u0624"'})]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Audio capture"}),": Microphones spoken command capture \u06a9\u0631\u062a\u06cc \u06c1\u06cc\u06ba"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Speech recognition"}),": OpenAI Whisper speech \u06a9\u0648 text \u0645\u06cc\u06ba transcribe \u06a9\u0631\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Text output"}),': "table \u0633\u06d2 \u0633\u0631\u062e \u06a9\u067e \u0627\u0679\u06be\u0627\u0624"']}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"step-2-cognitive-planning",children:"Step 2: Cognitive Planning"}),"\n",(0,o.jsx)(e.p,{children:"Transcribed text cognitive planning \u06a9\u06cc \u0637\u0631\u0641 \u0633\u06d2 process \u06c1\u0648\u062a\u0627 \u06c1\u06d2:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Intent understanding"}),": Robot \u0633\u0645\u062c\u06be\u062a\u0627 \u06c1\u06d2 \u06a9\u06c1 \u0627\u0633\u06d2 specific object pick up \u06a9\u0631\u0646\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Goal decomposition"}),": Task sub-goals \u0645\u06cc\u06ba break \u06c1\u0648\u062a\u0627 \u06c1\u06d2:","\n",(0,o.jsxs)(e.ol,{children:["\n",(0,o.jsx)(e.li,{children:"Table \u067e\u0631 navigate \u06a9\u0631\u06cc\u06ba"}),"\n",(0,o.jsx)(e.li,{children:"Red cup identify \u06a9\u0631\u06cc\u06ba"}),"\n",(0,o.jsx)(e.li,{children:"Cup pick up \u06a9\u0631\u06cc\u06ba"}),"\n",(0,o.jsx)(e.li,{children:"Successful grasp verify \u06a9\u0631\u06cc\u06ba"}),"\n"]}),"\n"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Action sequence generation"}),": \u06c1\u0631 sub-goal \u06a9\u06d2 \u0644\u06cc\u06d2 ROS 2 actions generate \u06c1\u0648\u062a\u06d2 \u06c1\u06cc\u06ba"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"step-3-path-planning",children:"Step 3: Path Planning"}),"\n",(0,o.jsx)(e.p,{children:"Cognitive plan table \u067e\u0631 navigation \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u06cc \u06c1\u06d2:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Target location"}),": Table location identify \u06c1\u0648\u062a\u06cc \u06c1\u06d2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Path generation"}),": Current position \u0633\u06d2 table \u062a\u06a9 path plan \u06c1\u0648\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Obstacle consideration"}),": Known obstacles path planning \u0645\u06cc\u06ba consider \u06c1\u0648\u062a\u06d2 \u06c1\u06cc\u06ba"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Path validation"}),": Path feasibility \u06a9\u06d2 \u0644\u06cc\u06d2 check \u06c1\u0648\u062a\u0627 \u06c1\u06d2"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"step-4-obstacle-navigation",children:"Step 4: Obstacle Navigation"}),"\n",(0,o.jsx)(e.p,{children:"Robot table \u067e\u0631 navigate \u06a9\u0631\u062a\u0627 \u06c1\u06d2:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Perception"}),": Sensors environment \u0645\u06cc\u06ba obstacles detect \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Path adjustment"}),": Path adjust \u06c1\u0648\u062a\u0627 \u06c1\u06d2 obstacles avoid \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Navigation execution"}),": ROS 2 navigation actions robot move \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Progress monitoring"}),": Navigation progress monitor \u0627\u0648\u0631 adjust \u06c1\u0648\u062a\u0627 \u06c1\u06d2"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"step-5-object-identification",children:"Step 5: Object Identification"}),"\n",(0,o.jsx)(e.p,{children:"Table \u067e\u0631\u060c computer vision target identify \u06a9\u0631\u062a\u0627 \u06c1\u06d2:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Visual perception"}),": Cameras table \u06a9\u06cc images capture \u06a9\u0631\u062a\u06cc \u06c1\u06cc\u06ba"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Object detection"}),": Computer vision table \u067e\u0631 objects detect \u06a9\u0631\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Object classification"}),": Objects classified \u06c1\u0648\u062a\u06d2 \u06c1\u06cc\u06ba (cup\u060c bottle\u060c etc.)"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Target selection"}),": Red cup identify \u0627\u0648\u0631 select \u06c1\u0648\u062a\u0627 \u06c1\u06d2"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"step-6-object-manipulation",children:"Step 6: Object Manipulation"}),"\n",(0,o.jsx)(e.p,{children:"Robot identified object manipulate \u06a9\u0631\u062a\u0627 \u06c1\u06d2:"}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Grasp planning"}),": Cup \u06a9\u06d2 \u0644\u06cc\u06d2 grasp pose plan \u06c1\u0648\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Arm movement"}),": Robot \u06a9\u06cc arm grasp pose \u067e\u0631 move \u06a9\u0631\u062a\u06cc \u06c1\u06d2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Grasp execution"}),": Robot cup grasp \u06a9\u0631\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Verification"}),": Successful grasp verify \u06c1\u0648\u062a\u0627 \u06c1\u06d2"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"vla-components-\u06a9\u06cc-integration",children:"VLA Components \u06a9\u06cc Integration"}),"\n",(0,o.jsx)(e.p,{children:"\u06cc\u06c1 capstone demonstrate \u06a9\u0631\u062a\u0627 \u06c1\u06d2 \u06a9\u06c1 \u062a\u0645\u0627\u0645 VLA components \u06a9\u06cc\u0633\u06d2 integrate \u06c1\u0648\u062a\u06d2 \u06c1\u06cc\u06ba:"}),"\n",(0,o.jsx)(e.h3,{id:"voice-input-integration",children:"Voice Input Integration"}),"\n",(0,o.jsx)(e.p,{children:"Voice commands entire pipeline initiate \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# Whisper Integration Pattern in Capstone Context\n# This demonstrates how voice input initiates the capstone project\n\nimport whisper\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass CapstoneVoiceInput(Node):\n    """\n    Voice input component for capstone project.\n    Demonstrates Whisper integration in complete VLA pipeline.\n    """\n    \n    def __init__(self):\n        super().__init__(\'capstone_voice_input\')\n        \n        # Initialize Whisper (conceptual pattern)\n        self.whisper_model = whisper.load_model("base")\n        \n        # Publisher for sending commands to cognitive planning\n        self.command_publisher = self.create_publisher(\n            String,\n            \'voice_command\',\n            10\n        )\n    \n    def process_voice_command(self, audio_data):\n        """\n        Process voice command and publish to cognitive planning.\n        This initiates the complete VLA pipeline.\n        """\n        # Transcribe audio to text using Whisper\n        result = self.whisper_model.transcribe(audio_data)\n        command_text = result["text"]\n        \n        # Publish command to cognitive planning system\n        msg = String()\n        msg.data = command_text\n        self.command_publisher.publish(msg)\n        \n        self.get_logger().info(f\'Voice command received: {command_text}\')\n        return command_text\n'})}),"\n",(0,o.jsx)(e.h3,{id:"cognitive-planning-integration",children:"Cognitive Planning Integration"}),"\n",(0,o.jsx)(e.p,{children:"Cognitive planning commands translate \u06a9\u0631\u062a\u06cc \u06c1\u06d2 action sequences \u0645\u06cc\u06ba:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# LLM Cognitive Planning Integration Pattern in Capstone Context\n# This demonstrates how cognitive planning generates action sequences\n\nclass CapstoneCognitivePlanner:\n    """\n    Cognitive planning component for capstone project.\n    Demonstrates LLM integration in complete VLA pipeline.\n    """\n    \n    def __init__(self, llm_client):\n        self.llm_client = llm_client\n    \n    def plan_capstone_task(self, voice_command, robot_context):\n        """\n        Generate action plan for capstone task.\n        This demonstrates cognitive planning in complete VLA pipeline.\n        """\n        # Create planning prompt\n        prompt = f"""\n        Translate this voice command into a robot action plan:\n        Command: "{voice_command}"\n        \n        Robot context: {robot_context}\n        \n        Generate a sequence of actions:\n        1. Navigation to target location\n        2. Object identification\n        3. Object manipulation\n        \n        Output the action sequence in ROS 2 action format.\n        """\n        \n        # Generate plan using LLM\n        response = self.llm_client.generate(prompt)\n        \n        # Parse response into action sequence\n        action_sequence = self.parse_plan(response)\n        \n        return action_sequence\n    \n    def parse_plan(self, llm_response):\n        """\n        Parse LLM response into ROS 2 action sequence.\n        This demonstrates how cognitive plans become executable actions.\n        """\n        # Parse structured response\n        # Extract navigation, perception, and manipulation actions\n        actions = []\n        # ... parsing logic ...\n        return actions\n'})}),"\n",(0,o.jsx)(e.h3,{id:"ros-2-action-generation-integration",children:"ROS 2 Action Generation Integration"}),"\n",(0,o.jsx)(e.p,{children:"ROS 2 actions cognitive plan execute \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-python",children:'# ROS 2 Action Generation Integration Pattern in Capstone Context\n# This demonstrates how cognitive plans become ROS 2 actions\n\nfrom rclpy.action import ActionClient\nfrom navigation_msgs.action import NavigateToPose\nfrom manipulation_msgs.action import PickPlace\nfrom perception_msgs.action import DetectObjects\n\nclass CapstoneActionExecutor(Node):\n    """\n    Action execution component for capstone project.\n    Demonstrates ROS 2 action generation in complete VLA pipeline.\n    """\n    \n    def __init__(self):\n        super().__init__(\'capstone_action_executor\')\n        \n        # Action clients for different capabilities\n        self.nav_client = ActionClient(self, NavigateToPose, \'navigate_to_pose\')\n        self.manip_client = ActionClient(self, PickPlace, \'pick_place\')\n        self.perception_client = ActionClient(self, DetectObjects, \'detect_objects\')\n    \n    def execute_capstone_plan(self, cognitive_plan):\n        """\n        Execute complete capstone plan.\n        This demonstrates ROS 2 action execution in complete VLA pipeline.\n        """\n        # Execute navigation actions\n        for nav_action in cognitive_plan.navigation_actions:\n            self.execute_navigation(nav_action)\n        \n        # Execute perception actions\n        for perception_action in cognitive_plan.perception_actions:\n            self.execute_perception(perception_action)\n        \n        # Execute manipulation actions\n        for manip_action in cognitive_plan.manipulation_actions:\n            self.execute_manipulation(manip_action)\n    \n    def execute_navigation(self, action):\n        """Execute navigation action."""\n        goal = NavigateToPose.Goal()\n        goal.pose = action.target_pose\n        self.nav_client.send_goal_async(goal)\n    \n    def execute_manipulation(self, action):\n        """Execute manipulation action."""\n        goal = PickPlace.Goal()\n        goal.object_id = action.object_id\n        goal.pick_pose = action.pick_pose\n        self.manip_client.send_goal_async(goal)\n'})}),"\n",(0,o.jsx)(e.h2,{id:"previous-modules-\u0633\u06d2-cross-references",children:"Previous Modules \u0633\u06d2 Cross-References"}),"\n",(0,o.jsx)(e.p,{children:"\u06cc\u06c1 capstone project previous modules \u0633\u06d2 concepts \u067e\u0631 build \u06a9\u0631\u062a\u0627 \u06c1\u06d2:"}),"\n",(0,o.jsx)(e.h3,{id:"\u0645\u0627\u0688\u06cc\u0648\u0644-2-simulation",children:"\u0645\u0627\u0688\u06cc\u0648\u0644 2: Simulation"}),"\n",(0,o.jsxs)(e.p,{children:["Capstone project simulated environment \u0645\u06cc\u06ba run \u06c1\u0648\u062a\u0627 \u06c1\u06d2\u060c demonstrating \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2 \u06a9\u06c1 ",(0,o.jsx)(e.a,{href:"/ur/modules/module-2-digital-twins-simulation/simulation-fundamentals",children:"\u0645\u0627\u0688\u06cc\u0648\u0644 2: Digital Twins - Simulation & Sensors"})," \u06a9\u06cc\u0633\u06d2 safe testing \u0627\u0648\u0631 development enable \u06a9\u0631\u062a\u0627 \u06c1\u06d2 VLA systems \u06a9\u06cc\u06d4 Simulation allow \u06a9\u0631\u062a\u0627 \u06c1\u06d2:"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Safe testing"}),": VLA systems test \u06a9\u0631\u0646\u0627 physical risk \u06a9\u06d2 \u0628\u063a\u06cc\u0631"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Rapid iteration"}),": Quickly different scenarios test \u06a9\u0631\u0646\u0627"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Environment control"}),": Consistent test conditions create \u06a9\u0631\u0646\u0627"]}),"\n"]}),"\n",(0,o.jsx)(e.h3,{id:"\u0645\u0627\u0688\u06cc\u0648\u0644-3-perception-\u0627\u0648\u0631-navigation",children:"\u0645\u0627\u0688\u06cc\u0648\u0644 3: Perception \u0627\u0648\u0631 Navigation"}),"\n",(0,o.jsxs)(e.p,{children:["Capstone project perception \u0627\u0648\u0631 navigation capabilities \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u0627 \u06c1\u06d2 ",(0,o.jsx)(e.a,{href:"/ur/modules/module-3-ai-robot-brain/nav2-path-planning",children:"\u0645\u0627\u0688\u06cc\u0648\u0644 3: The AI-Robot Brain (NVIDIA Isaac\u2122)"})," \u0633\u06d2\u060c including:"]}),"\n",(0,o.jsxs)(e.ul,{children:["\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Computer vision"}),": Object identification \u0627\u0648\u0631 classification"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Path planning"}),": Navigation path generation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Obstacle avoidance"}),": Dynamic obstacle navigation"]}),"\n",(0,o.jsxs)(e.li,{children:[(0,o.jsx)(e.strong,{children:"Sensor integration"}),": Perception \u06a9\u06d2 \u0644\u06cc\u06d2 cameras \u0627\u0648\u0631 sensors \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u0646\u0627"]}),"\n"]}),"\n",(0,o.jsx)(e.h2,{id:"complete-integration-flow-diagram",children:"Complete Integration Flow Diagram"}),"\n",(0,o.jsx)(e.p,{children:"\u062f\u0631\u062c \u0630\u06cc\u0644 diagram complete capstone project flow illustrate \u06a9\u0631\u062a\u0627 \u06c1\u06d2:"}),"\n",(0,o.jsx)(e.pre,{children:(0,o.jsx)(e.code,{className:"language-mermaid",children:"flowchart TD\n    A[Voice Command] --\x3e B[Whisper Transcription]\n    B --\x3e C[Cognitive Planning]\n    C --\x3e D[Action Sequence]\n    D --\x3e E[Path Planning]\n    E --\x3e F[Navigation Actions]\n    F --\x3e G[Obstacle Navigation]\n    G --\x3e H[Object Detection]\n    H --\x3e I[Object Identification]\n    I --\x3e J[Grasp Planning]\n    J --\x3e K[Manipulation Actions]\n    K --\x3e L[Task Completion]\n    \n    style A fill:#e1f5ff\n    style C fill:#fff4e1\n    style E fill:#e8f5e9\n    style L fill:#fce4ec\n"})}),"\n",(0,o.jsx)(e.p,{children:"\u06cc\u06c1 diagram \u062f\u06a9\u06be\u0627\u062a\u0627 \u06c1\u06d2 \u06a9\u06c1 \u062a\u0645\u0627\u0645 VLA components \u06a9\u06cc\u0633\u06d2 integrate \u06c1\u0648\u062a\u06d2 \u06c1\u06cc\u06ba complete autonomous behavior \u0645\u06cc\u06ba\u06d4"}),"\n",(0,o.jsx)(e.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(e.p,{children:"Capstone project complete VLA pipeline demonstrate \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c voice input\u060c cognitive planning\u060c path planning\u060c obstacle navigation\u060c object identification\u060c \u0627\u0648\u0631 manipulation integrate \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2 cohesive autonomous behavior \u0645\u06cc\u06ba\u06d4 \u06cc\u06c1 project \u062f\u06a9\u06be\u0627\u062a\u0627 \u06c1\u06d2 \u06a9\u06c1 \u062a\u0645\u0627\u0645 VLA concepts \u06a9\u06cc\u0633\u06d2 \u0645\u0644 \u06a9\u0631 \u06a9\u0627\u0645 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba\u060c simple voice command \u0633\u06d2 complex robot behavior \u062a\u06a9\u06d4 \u06cc\u06c1 integration \u0633\u0645\u062c\u06be\u0646\u0627 essential \u06c1\u06d2 comprehending \u06a9\u06d2 \u0644\u06cc\u06d2 \u06a9\u06c1 VLA systems \u06a9\u06cc\u0633\u06d2 natural language robot control enable \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba\u06d4"}),"\n",(0,o.jsx)(e.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(e.p,{children:["\u0627\u0628 \u062c\u0628 \u06a9\u06c1 \u0622\u067e capstone project \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 complete VLA pipeline \u0633\u0645\u062c\u06be \u06af\u0626\u06d2 \u06c1\u06cc\u06ba\u060c ",(0,o.jsx)(e.a,{href:"/ur/modules/module-4-vision-language-action/module-integration",children:"Module Integration"})," \u067e\u0631 \u062c\u0627\u0626\u06cc\u06ba \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 \u06a9\u06c1 VLA concepts \u06a9\u06cc\u0633\u06d2 connect \u06c1\u0648\u062a\u06d2 \u06c1\u06cc\u06ba \u0627\u0648\u0631 build \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba previous modules \u067e\u0631\u06d4"]})]})}function d(n={}){const{wrapper:e}={...(0,a.R)(),...n.components};return e?(0,o.jsx)(e,{...n,children:(0,o.jsx)(p,{...n})}):p(n)}},8453:(n,e,i)=>{i.d(e,{R:()=>s,x:()=>l});var t=i(6540);const o={},a=t.createContext(o);function s(n){const e=t.useContext(a);return t.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function l(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(o):n.components||o:s(n.components),t.createElement(a.Provider,{value:e},n.children)}}}]);