"use strict";(globalThis.webpackChunk=globalThis.webpackChunk||[]).push([[2420],{6524:(e,n,s)=>{s.r(n),s.d(n,{assets:()=>l,contentTitle:()=>a,default:()=>m,frontMatter:()=>t,metadata:()=>i,toc:()=>c});const i=JSON.parse('{"id":"modules/module-2-digital-twins-simulation/sensor-integration","title":"Sensor Integration","description":"Sensor types \u0627\u0648\u0631 ROS 2 integration humanoid robots \u06a9\u06d2 \u0644\u06cc\u06d2\u060c vision\u060c proprioception\u060c \u0627\u0648\u0631 tactile sensors \u06a9\u0627 \u0627\u062d\u0627\u0637\u06c1 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/modules/module-2-digital-twins-simulation/sensor-integration.md","sourceDirName":"modules/module-2-digital-twins-simulation","slug":"/modules/module-2-digital-twins-simulation/sensor-integration","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-2-digital-twins-simulation/sensor-integration","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"sensors","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/sensors"},{"inline":true,"label":"ros2-integration","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/ros-2-integration"},{"inline":true,"label":"sensor-data-flow","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/sensor-data-flow"},{"inline":true,"label":"vision","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/vision"},{"inline":true,"label":"proprioception","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/proprioception"},{"inline":true,"label":"tactile","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/tactile"}],"version":"current","sidebarPosition":4,"frontMatter":{"id":"sensor-integration","title":"Sensor Integration","sidebar_position":4,"description":"Sensor types \u0627\u0648\u0631 ROS 2 integration humanoid robots \u06a9\u06d2 \u0644\u06cc\u06d2\u060c vision\u060c proprioception\u060c \u0627\u0648\u0631 tactile sensors \u06a9\u0627 \u0627\u062d\u0627\u0637\u06c1 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4","tags":["sensors","ros2-integration","sensor-data-flow","vision","proprioception","tactile"],"learning_objectives":["lo-006"],"topic_category":"sensor"},"sidebar":"textbookSidebar","previous":{"title":"Simulation Fundamentals","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-2-digital-twins-simulation/simulation-fundamentals"},"next":{"title":"Humanoid Applications","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-2-digital-twins-simulation/humanoid-applications"}}');var o=s(4848),r=s(8453);const t={id:"sensor-integration",title:"Sensor Integration",sidebar_position:4,description:"Sensor types \u0627\u0648\u0631 ROS 2 integration humanoid robots \u06a9\u06d2 \u0644\u06cc\u06d2\u060c vision\u060c proprioception\u060c \u0627\u0648\u0631 tactile sensors \u06a9\u0627 \u0627\u062d\u0627\u0637\u06c1 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4",tags:["sensors","ros2-integration","sensor-data-flow","vision","proprioception","tactile"],learning_objectives:["lo-006"],topic_category:"sensor"},a="Sensor Integration",l={},c=[{value:"Humanoid Robots \u0645\u06cc\u06ba Sensor Types",id:"humanoid-robots-\u0645\u06cc\u06ba-sensor-types",level:2},{value:"Vision Sensors (Cameras)",id:"vision-sensors-cameras",level:3},{value:"Vision Sensors \u06a9\u06cc\u0633\u06d2 \u06a9\u0627\u0645 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba",id:"vision-sensors-\u06a9\u06cc\u0633\u06d2-\u06a9\u0627\u0645-\u06a9\u0631\u062a\u06d2-\u06c1\u06cc\u06ba",level:4},{value:"Humanoid Robotics Use Cases",id:"humanoid-robotics-use-cases",level:4},{value:"Example: Camera Data Processing",id:"example-camera-data-processing",level:4},{value:"Proprioceptive Sensors (IMUs\u060c Joint Encoders)",id:"proprioceptive-sensors-imus-joint-encoders",level:3},{value:"Inertial Measurement Units (IMUs)",id:"inertial-measurement-units-imus",level:4},{value:"Joint Encoders",id:"joint-encoders",level:4},{value:"Humanoid Robotics Use Cases",id:"humanoid-robotics-use-cases-1",level:4},{value:"Tactile Sensors",id:"tactile-sensors",level:3},{value:"Tactile Sensors \u06a9\u06cc\u0633\u06d2 \u06a9\u0627\u0645 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba",id:"tactile-sensors-\u06a9\u06cc\u0633\u06d2-\u06a9\u0627\u0645-\u06a9\u0631\u062a\u06d2-\u06c1\u06cc\u06ba",level:4},{value:"Humanoid Robotics Use Cases",id:"humanoid-robotics-use-cases-2",level:4},{value:"ROS 2 \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 Sensor Data Flow",id:"ros-2-\u06a9\u06d2-\u0630\u0631\u06cc\u0639\u06d2-sensor-data-flow",level:2},{value:"Sensor Data Flow Architecture",id:"sensor-data-flow-architecture",level:3},{value:"Sensors \u06a9\u06cc\u0633\u06d2 Data Publish \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba Topics \u067e\u0631",id:"sensors-\u06a9\u06cc\u0633\u06d2-data-publish-\u06a9\u0631\u062a\u06d2-\u06c1\u06cc\u06ba-topics-\u067e\u0631",level:3},{value:"Example: Sensor Data Publishing",id:"example-sensor-data-publishing",level:3},{value:"Sensors Robot Perception \u0627\u0648\u0631 Decision-Making \u06a9\u0648 \u06a9\u06cc\u0633\u06d2 Enable \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba",id:"sensors-robot-perception-\u0627\u0648\u0631-decision-making-\u06a9\u0648-\u06a9\u06cc\u0633\u06d2-enable-\u06a9\u0631\u062a\u06d2-\u06c1\u06cc\u06ba",level:2},{value:"Perception Pipeline",id:"perception-pipeline",level:3},{value:"Example: Multi-Sensor Perception",id:"example-multi-sensor-perception",level:3},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2},{value:"Related Content",id:"related-content",level:2}];function d(e){const n={a:"a",code:"code",em:"em",h1:"h1",h2:"h2",h3:"h3",h4:"h4",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,r.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"sensor-integration",children:"Sensor Integration"})}),"\n",(0,o.jsx)(n.p,{children:"Sensors primary interface \u06c1\u06cc\u06ba humanoid robots \u0627\u0648\u0631 \u0627\u0646 \u06a9\u06d2 environment \u06a9\u06d2 \u062f\u0631\u0645\u06cc\u0627\u0646\u06d4 \u0648\u06c1 data \u0641\u0631\u0627\u06c1\u0645 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba \u062c\u0648 robots \u06a9\u0648 world perceive \u06a9\u0631\u0646\u06d2\u060c \u0627\u067e\u0646\u06cc state \u0633\u0645\u062c\u06be\u0646\u06d2\u060c \u0627\u0648\u0631 decisions \u0628\u0646\u0627\u0646\u06d2 \u06a9\u06d2 \u0642\u0627\u0628\u0644 \u0628\u0646\u0627\u062a\u0627 \u06c1\u06d2\u06d4 \u06cc\u06c1 \u0633\u0645\u062c\u06be\u0646\u0627 \u06a9\u06c1 sensors \u06a9\u06cc\u0633\u06d2 ROS 2 \u06a9\u06d2 \u0633\u0627\u062a\u06be integrate \u06c1\u0648\u062a\u06d2 \u06c1\u06cc\u06ba essential \u06c1\u06d2 functional humanoid robots \u0628\u0646\u0627\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2\u06d4"}),"\n",(0,o.jsx)(n.h2,{id:"humanoid-robots-\u0645\u06cc\u06ba-sensor-types",children:"Humanoid Robots \u0645\u06cc\u06ba Sensor Types"}),"\n",(0,o.jsx)(n.p,{children:"Humanoid robots multiple sensor types \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba \u0627\u067e\u0646\u06d2 environment \u0627\u0648\u0631 internal state \u06a9\u06d2 \u0628\u0627\u0631\u06d2 \u0645\u06cc\u06ba information \u062c\u0645\u0639 \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2\u06d4 \u06c1\u0631 sensor type different information \u0641\u0631\u0627\u06c1\u0645 \u06a9\u0631\u062a\u0627 \u06c1\u06d2 \u062c\u0648 robot perception \u0627\u0648\u0631 decision-making \u0645\u06cc\u06ba contribute \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u06d4"}),"\n",(0,o.jsx)(n.h3,{id:"vision-sensors-cameras",children:"Vision Sensors (Cameras)"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Vision sensors"})," environment \u0633\u06d2 visual information capture \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba\u060c robots \u06a9\u0648 \u062f\u06cc\u06a9\u06be\u0646\u06d2 \u0627\u0648\u0631 \u0627\u067e\u0646\u06d2 surroundings \u0633\u0645\u062c\u06be\u0646\u06d2 \u06a9\u06d2 \u0642\u0627\u0628\u0644 \u0628\u0646\u0627\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4"]}),"\n",(0,o.jsx)(n.h4,{id:"vision-sensors-\u06a9\u06cc\u0633\u06d2-\u06a9\u0627\u0645-\u06a9\u0631\u062a\u06d2-\u06c1\u06cc\u06ba",children:"Vision Sensors \u06a9\u06cc\u0633\u06d2 \u06a9\u0627\u0645 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba"}),"\n",(0,o.jsx)(n.p,{children:"Cameras light capture \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba \u0627\u0648\u0631 \u0627\u0633\u06d2 digital images \u0645\u06cc\u06ba convert \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Image capture"}),": Light lens \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 \u062f\u0627\u062e\u0644 \u06c1\u0648\u062a\u06cc \u06c1\u06d2\u060c image sensor \u06a9\u0648 hit \u06a9\u0631\u062a\u06cc \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Digital conversion"}),": Sensor light \u06a9\u0648 pixel values \u0645\u06cc\u06ba convert \u06a9\u0631\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Image processing"}),": Raw data usable format \u0645\u06cc\u06ba processed \u06c1\u0648\u062a\u06cc \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Data publishing"}),": Images ROS 2 topics \u067e\u0631 published \u06c1\u0648\u062a\u06cc \u06c1\u06cc\u06ba"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"humanoid-robotics-use-cases",children:"Humanoid Robotics Use Cases"}),"\n",(0,o.jsx)(n.p,{children:"Vision sensors humanoid robots \u06a9\u0648 enable \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Navigate"})," landmarks \u0627\u0648\u0631 obstacles recognize \u06a9\u0631 \u06a9\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Objects manipulate"})," items identify \u0627\u0648\u0631 locate \u06a9\u0631 \u06a9\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Humans \u06a9\u06d2 \u0633\u0627\u062a\u06be interact"})," faces \u0627\u0648\u0631 gestures recognize \u06a9\u0631 \u06a9\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Scenes understand"})," spatial relationships analyze \u06a9\u0631 \u06a9\u06d2"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"example-camera-data-processing",children:"Example: Camera Data Processing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Example: Processing camera data in humanoid robotics\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image\nfrom cv_bridge import CvBridge\nimport cv2\n\nclass VisionProcessingNode(Node):\n    """ROS 2 node for processing camera data in humanoid robot"""\n    \n    def __init__(self):\n        super().__init__(\'vision_processing_node\')\n        \n        # Subscribe to camera topic\n        self.subscription = self.create_subscription(\n            Image,\n            \'/camera/image_raw\',  # Camera topic\n            self.image_callback,\n            10\n        )\n        \n        # Publish processed results\n        self.publisher = self.create_publisher(\n            Image,\n            \'/vision/processed_image\',\n            10\n        )\n        \n        self.bridge = CvBridge()\n        \n    def image_callback(self, msg):\n        """Process incoming camera image"""\n        # Convert ROS image message to OpenCV format\n        cv_image = self.bridge.imgmsg_to_cv2(msg, \'bgr8\')\n        \n        # Process image (e.g., object detection, edge detection)\n        processed_image = self.process_image(cv_image)\n        \n        # Convert back to ROS message\n        ros_image = self.bridge.cv2_to_imgmsg(processed_image, \'bgr8\')\n        \n        # Publish processed image\n        self.publisher.publish(ros_image)\n        \n    def process_image(self, image):\n        """Process image for humanoid robot perception"""\n        # Example: Edge detection for navigation\n        gray = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n        edges = cv2.Canny(gray, 50, 150)\n        return edges\n'})}),"\n",(0,o.jsx)(n.p,{children:"\u06cc\u06c1 example \u062f\u06a9\u06be\u0627\u062a\u0627 \u06c1\u06d2 \u06a9\u06c1 vision sensors \u06a9\u06cc\u0633\u06d2 ROS 2 \u06a9\u06d2 \u0633\u0627\u062a\u06be integrate \u06c1\u0648\u062a\u06d2 \u06c1\u06cc\u06ba:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Subscribing"})," camera topic (",(0,o.jsx)(n.code,{children:"/camera/image_raw"}),") \u06a9\u0648"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Processing"})," image data perception \u06a9\u06d2 \u0644\u06cc\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Publishing"})," results \u062f\u0648\u0633\u0631\u06d2 nodes \u06a9\u0648"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"proprioceptive-sensors-imus-joint-encoders",children:"Proprioceptive Sensors (IMUs\u060c Joint Encoders)"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Proprioceptive sensors"})," robot \u06a9\u06cc internal state measure \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba\u060c body position\u060c orientation\u060c \u0627\u0648\u0631 movement \u06a9\u06d2 \u0628\u0627\u0631\u06d2 \u0645\u06cc\u06ba information \u0641\u0631\u0627\u06c1\u0645 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4"]}),"\n",(0,o.jsx)(n.h4,{id:"inertial-measurement-units-imus",children:"Inertial Measurement Units (IMUs)"}),"\n",(0,o.jsx)(n.p,{children:"IMUs orientation \u0627\u0648\u0631 acceleration measure \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Gyroscopes"}),": Angular velocity measure \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba (rotation rate)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Accelerometers"}),": Linear acceleration measure \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Magnetometers"}),": Magnetic field measure \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba (heading \u06a9\u06d2 \u0644\u06cc\u06d2)"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"IMUs critical information \u0641\u0631\u0627\u06c1\u0645 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Balance control"})," \u06a9\u06d2 \u0644\u06cc\u06d2: Robot orientation \u0633\u0645\u062c\u06be\u0646\u0627"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Motion planning"})," \u06a9\u06d2 \u0644\u06cc\u06d2: Body movement track \u06a9\u0631\u0646\u0627"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Fall detection"})," \u06a9\u06d2 \u0644\u06cc\u06d2: Detect \u06a9\u0631\u0646\u0627 \u062c\u0628 robot falling \u06c1\u0648"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"joint-encoders",children:"Joint Encoders"}),"\n",(0,o.jsx)(n.p,{children:"Joint encoders joint angles measure \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Position feedback"}),": \u06c1\u0631 joint \u06a9\u0627 current angle"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Velocity feedback"}),": Joint movement \u06a9\u06cc rate"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Torque feedback"}),": Joints \u067e\u0631 applied force"]}),"\n"]}),"\n",(0,o.jsx)(n.p,{children:"Joint encoders enable \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Precise control"}),": Exact joint positions \u062c\u0627\u0646\u0646\u0627"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Coordination"}),": Multiple joints synchronize \u06a9\u0631\u0646\u0627"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safety"}),": Joint limits \u0627\u0648\u0631 collisions detect \u06a9\u0631\u0646\u0627"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"humanoid-robotics-use-cases-1",children:"Humanoid Robotics Use Cases"}),"\n",(0,o.jsx)(n.p,{children:"Proprioceptive sensors humanoid robots \u06a9\u0648 enable \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Balance maintain"})," body orientation \u0627\u0648\u0631 joint positions sense \u06a9\u0631 \u06a9\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Movement coordinate"})," \u062a\u0645\u0627\u0645 joint states track \u06a9\u0631 \u06a9\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Falls detect"})," acceleration \u0627\u0648\u0631 orientation monitor \u06a9\u0631 \u06a9\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Motions plan"})," current body configuration \u0633\u0645\u062c\u06be \u06a9\u0631"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"tactile-sensors",children:"Tactile Sensors"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"Tactile sensors"})," contact \u0627\u0648\u0631 force detect \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba\u060c robots \u06a9\u0648 touch sense \u06a9\u0631\u0646\u06d2 \u0627\u0648\u0631 objects \u06a9\u06d2 \u0633\u0627\u062a\u06be interaction \u06a9\u06d2 \u0642\u0627\u0628\u0644 \u0628\u0646\u0627\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4"]}),"\n",(0,o.jsx)(n.h4,{id:"tactile-sensors-\u06a9\u06cc\u0633\u06d2-\u06a9\u0627\u0645-\u06a9\u0631\u062a\u06d2-\u06c1\u06cc\u06ba",children:"Tactile Sensors \u06a9\u06cc\u0633\u06d2 \u06a9\u0627\u0645 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba"}),"\n",(0,o.jsx)(n.p,{children:"Tactile sensors measure \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Contact detection"}),": \u06a9\u06cc\u0627 robot \u06a9\u0686\u06be touch \u06a9\u0631 \u0631\u06c1\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Force magnitude"}),": Contact \u06a9\u062a\u0646\u0627 hard \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Force direction"}),": Applied force \u06a9\u06cc direction"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Contact location"}),": Robot \u067e\u0631 \u06a9\u06c1\u0627\u06ba contact occur \u06c1\u0648\u062a\u0627 \u06c1\u06d2"]}),"\n"]}),"\n",(0,o.jsx)(n.h4,{id:"humanoid-robotics-use-cases-2",children:"Humanoid Robotics Use Cases"}),"\n",(0,o.jsx)(n.p,{children:"Tactile sensors humanoid robots \u06a9\u0648 enable \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Objects grasp"})," contact sense \u06a9\u0631 \u06a9\u06d2 \u0627\u0648\u0631 grip force adjust \u06a9\u0631 \u06a9\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Collisions detect"})," unexpected contact sense \u06a9\u0631 \u06a9\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Safely interact"})," humans \u06a9\u06d2 \u0633\u0627\u062a\u06be touch sense \u06a9\u0631 \u06a9\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Objects manipulate"})," contact forces feel \u06a9\u0631 \u06a9\u06d2"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"ros-2-\u06a9\u06d2-\u0630\u0631\u06cc\u0639\u06d2-sensor-data-flow",children:"ROS 2 \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 Sensor Data Flow"}),"\n",(0,o.jsxs)(n.p,{children:["Sensor data ROS 2 \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 flow \u06a9\u0631\u062a\u0627 \u06c1\u06d2 ",(0,o.jsx)(n.strong,{children:"publish-subscribe pattern"})," \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u060c \u062c\u06cc\u0633\u0627 \u06a9\u06c1 ",(0,o.jsx)(n.a,{href:"/ur/modules/module-1-ros2-nervous-system/communication-patterns#publish-subscribe-pattern-topics",children:"\u0645\u0627\u0688\u06cc\u0648\u0644 1 \u06a9\u06d2 communication patterns"})," \u0645\u06cc\u06ba discuss \u06a9\u06cc\u0627 \u06af\u06cc\u0627 \u06c1\u06d2\u06d4 \u06cc\u06c1 decoupled\u060c asynchronous communication \u06a9\u0648 \u0645\u0645\u06a9\u0646 \u0628\u0646\u0627\u062a\u0627 \u06c1\u06d2 sensors \u0627\u0648\u0631 processing nodes \u06a9\u06d2 \u062f\u0631\u0645\u06cc\u0627\u0646\u06d4"]}),"\n",(0,o.jsx)(n.h3,{id:"sensor-data-flow-architecture",children:"Sensor Data Flow Architecture"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:"graph LR\n    A[Vision Sensor] --\x3e|Publish| B[/camera/image_raw]\n    C[IMU Sensor] --\x3e|Publish| D[/imu/data]\n    E[Joint Encoder] --\x3e|Publish| F[/joint_states]\n    G[Tactile Sensor] --\x3e|Publish| H[/tactile/contact]\n    \n    B --\x3e I[Vision Processing Node]\n    D --\x3e J[Balance Control Node]\n    F --\x3e K[Motion Planning Node]\n    H --\x3e L[Grasp Control Node]\n    \n    I --\x3e M[Perception System]\n    J --\x3e M\n    K --\x3e M\n    L --\x3e M\n    M --\x3e N[Decision Making]\n    \n    style A fill:#e1f5ff\n    style C fill:#fff4e1\n    style E fill:#e8f5e9\n    style G fill:#fce4ec\n    style M fill:#f3e5f5\n    style N fill:#e0f2f1\n"})}),"\n",(0,o.jsx)(n.p,{children:(0,o.jsx)(n.em,{children:"Figure 1: Sensor data flow ROS 2 topics \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2\u060c \u062f\u06a9\u06be\u0627\u062a\u0627 \u06c1\u06d2 \u06a9\u06c1 \u0645\u062e\u062a\u0644\u0641 sensor types \u06a9\u06cc\u0633\u06d2 data publish \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba \u062c\u0648 processing nodes consume \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba perception \u0627\u0648\u0631 decision-making \u06a9\u06d2 \u0644\u06cc\u06d2\u06d4"})}),"\n",(0,o.jsx)(n.h3,{id:"sensors-\u06a9\u06cc\u0633\u06d2-data-publish-\u06a9\u0631\u062a\u06d2-\u06c1\u06cc\u06ba-topics-\u067e\u0631",children:"Sensors \u06a9\u06cc\u0633\u06d2 Data Publish \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba Topics \u067e\u0631"}),"\n",(0,o.jsx)(n.p,{children:"Sensors ROS 2 topics \u067e\u0631 data publish \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba following pattern \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sensor node"})," hardware \u0633\u06d2 data capture \u06a9\u0631\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Data formatting"})," sensor data \u06a9\u0648 ROS 2 message format \u0645\u06cc\u06ba convert \u06a9\u0631\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Topic publishing"})," message \u06a9\u0648 topic \u067e\u0631 send \u06a9\u0631\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Subscriber nodes"})," data receive \u0627\u0648\u0631 process \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"example-sensor-data-publishing",children:"Example: Sensor Data Publishing"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:"# Example: Publishing sensor data to ROS 2 topics\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Imu, JointState\nfrom geometry_msgs.msg import Vector3\nimport math\n\nclass SensorNode(Node):\n    \"\"\"ROS 2 node for publishing sensor data from humanoid robot\"\"\"\n    \n    def __init__(self):\n        super().__init__('sensor_node')\n        \n        # Create publishers for different sensor types\n        self.imu_publisher = self.create_publisher(\n            Imu,\n            '/imu/data',\n            10\n        )\n        \n        self.joint_publisher = self.create_publisher(\n            JointState,\n            '/joint_states',\n            10\n        )\n        \n        # Timer to publish sensor data at regular intervals\n        self.timer = self.create_timer(0.01, self.publish_sensor_data)  # 100 Hz\n        \n    def publish_sensor_data(self):\n        \"\"\"Publish sensor data from humanoid robot\"\"\"\n        # Publish IMU data\n        imu_msg = Imu()\n        imu_msg.linear_acceleration.x = self.read_accelerometer_x()\n        imu_msg.linear_acceleration.y = self.read_accelerometer_y()\n        imu_msg.linear_acceleration.z = self.read_accelerometer_z()\n        imu_msg.angular_velocity.x = self.read_gyroscope_x()\n        imu_msg.angular_velocity.y = self.read_gyroscope_y()\n        imu_msg.angular_velocity.z = self.read_gyroscope_z()\n        self.imu_publisher.publish(imu_msg)\n        \n        # Publish joint encoder data\n        joint_msg = JointState()\n        joint_msg.name = ['shoulder', 'elbow', 'hip', 'knee', 'ankle']\n        joint_msg.position = [\n            self.read_joint_angle('shoulder'),\n            self.read_joint_angle('elbow'),\n            self.read_joint_angle('hip'),\n            self.read_joint_angle('knee'),\n            self.read_joint_angle('ankle')\n        ]\n        joint_msg.velocity = [0.0] * 5  # Joint velocities\n        joint_msg.effort = [0.0] * 5    # Joint torques\n        self.joint_publisher.publish(joint_msg)\n        \n    def read_accelerometer_x(self):\n        \"\"\"Read accelerometer X value (example)\"\"\"\n        # In real implementation, this would read from hardware\n        return 0.0\n        \n    def read_joint_angle(self, joint_name):\n        \"\"\"Read joint angle from encoder (example)\"\"\"\n        # In real implementation, this would read from hardware\n        return 0.0\n"})}),"\n",(0,o.jsx)(n.p,{children:"\u06cc\u06c1 example demonstrate \u06a9\u0631\u062a\u0627 \u06c1\u06d2:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Publishers create"})," \u06a9\u0631\u0646\u0627 different sensor types \u06a9\u06d2 \u0644\u06cc\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Data format"})," \u06a9\u0631\u0646\u0627 ROS 2 messages \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Regular intervals \u067e\u0631 publish"})," \u06a9\u0631\u0646\u0627 (proprioceptive sensors \u06a9\u06d2 \u0644\u06cc\u06d2 100 Hz)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multiple sensor types"})," different topics \u067e\u0631 publish \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"sensors-robot-perception-\u0627\u0648\u0631-decision-making-\u06a9\u0648-\u06a9\u06cc\u0633\u06d2-enable-\u06a9\u0631\u062a\u06d2-\u06c1\u06cc\u06ba",children:"Sensors Robot Perception \u0627\u0648\u0631 Decision-Making \u06a9\u0648 \u06a9\u06cc\u0633\u06d2 Enable \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba"}),"\n",(0,o.jsx)(n.p,{children:"Sensors raw data \u0641\u0631\u0627\u06c1\u0645 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba \u062c\u0648 robots \u06a9\u0648 enable \u06a9\u0631\u062a\u0627 \u06c1\u06d2:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Environment perceive"})," vision \u0627\u0648\u0631 tactile sensors \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Internal state understand"})," proprioceptive sensors \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Decisions make"})," combined sensor information \u06a9\u06cc \u0628\u0646\u06cc\u0627\u062f \u067e\u0631"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Actions execute"})," sensor feedback \u0633\u06d2 informed"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"perception-pipeline",children:"Perception Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"Perception pipeline sensor data process \u06a9\u0631\u062a\u0627 \u06c1\u06d2:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Raw sensor data"})," \u2192 Individual sensor readings"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sensor fusion"})," \u2192 Multiple sensors \u0633\u06d2 data combine \u06a9\u0631\u0646\u0627"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Feature extraction"})," \u2192 Relevant information identify \u06a9\u0631\u0646\u0627"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"State estimation"})," \u2192 Current situation \u0633\u0645\u062c\u06be\u0646\u0627"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Decision making"})," \u2192 Appropriate actions choose \u06a9\u0631\u0646\u0627"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"example-multi-sensor-perception",children:"Example: Multi-Sensor Perception"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Example: Using multiple sensors for perception and decision-making\nimport rclpy\nfrom rclpy.node import Node\nfrom sensor_msgs.msg import Image, Imu, JointState\n\nclass PerceptionNode(Node):\n    """ROS 2 node that fuses sensor data for perception"""\n    \n    def __init__(self):\n        super().__init__(\'perception_node\')\n        \n        # Subscribe to multiple sensor topics\n        self.create_subscription(Image, \'/camera/image_raw\', self.vision_callback, 10)\n        self.create_subscription(Imu, \'/imu/data\', self.imu_callback, 10)\n        self.create_subscription(JointState, \'/joint_states\', self.joint_callback, 10)\n        \n        # Store latest sensor data\n        self.latest_image = None\n        self.latest_imu = None\n        self.latest_joints = None\n        \n    def vision_callback(self, msg):\n        """Process vision data"""\n        self.latest_image = msg\n        self.update_perception()\n        \n    def imu_callback(self, msg):\n        """Process IMU data"""\n        self.latest_imu = msg\n        self.update_perception()\n        \n    def joint_callback(self, msg):\n        """Process joint encoder data"""\n        self.latest_joints = msg\n        self.update_perception()\n        \n    def update_perception(self):\n        """Fuse sensor data for perception and decision-making"""\n        if not all([self.latest_image, self.latest_imu, self.latest_joints]):\n            return\n            \n        # Fuse sensor data\n        robot_state = self.estimate_robot_state()\n        environment_state = self.analyze_environment()\n        \n        # Make decision based on perception\n        action = self.decide_action(robot_state, environment_state)\n        self.execute_action(action)\n        \n    def estimate_robot_state(self):\n        """Estimate robot state from proprioceptive sensors"""\n        # Use IMU for orientation\n        orientation = self.latest_imu.orientation\n        \n        # Use joint encoders for body configuration\n        joint_positions = self.latest_joints.position\n        \n        return {\n            \'orientation\': orientation,\n            \'joint_positions\': joint_positions\n        }\n        \n    def analyze_environment(self):\n        """Analyze environment from vision sensor"""\n        # Process image to understand environment\n        # (object detection, obstacle identification, etc.)\n        return {\'obstacles\': [], \'objects\': []}\n        \n    def decide_action(self, robot_state, environment_state):\n        """Make decision based on perception"""\n        # Decision-making logic using sensor data\n        return \'move_forward\'\n        \n    def execute_action(self, action):\n        """Execute decided action"""\n        self.get_logger().info(f\'Executing action: {action}\')\n'})}),"\n",(0,o.jsx)(n.p,{children:"\u06cc\u06c1 example \u062f\u06a9\u06be\u0627\u062a\u0627 \u06c1\u06d2 \u06a9\u06c1:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Multiple sensors"})," different types \u06a9\u06cc information \u0641\u0631\u0627\u06c1\u0645 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Sensor fusion"})," comprehensive understanding \u06a9\u06d2 \u0644\u06cc\u06d2 data combine \u06a9\u0631\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Perception"})," raw data \u0633\u06d2 meaningful information extract \u06a9\u0631\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Decision-making"})," perception \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u0627 \u06c1\u06d2 actions choose \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"Humanoid robots multiple sensor types \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba: vision sensors (cameras) visual perception \u06a9\u06d2 \u0644\u06cc\u06d2\u060c proprioceptive sensors (IMUs\u060c joint encoders) internal state \u06a9\u06d2 \u0644\u06cc\u06d2\u060c \u0627\u0648\u0631 tactile sensors contact detection \u06a9\u06d2 \u0644\u06cc\u06d2\u06d4 Sensor data ROS 2 topics \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 flow \u06a9\u0631\u062a\u0627 \u06c1\u06d2 publish-subscribe pattern \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u060c decoupled communication \u06a9\u0648 \u0645\u0645\u06a9\u0646 \u0628\u0646\u0627\u062a\u06d2 \u06c1\u0648\u0626\u06d2 sensors \u0627\u0648\u0631 processing nodes \u06a9\u06d2 \u062f\u0631\u0645\u06cc\u0627\u0646\u06d4 Sensors robot perception \u06a9\u0648 \u0645\u0645\u06a9\u0646 \u0628\u0646\u0627\u062a\u06d2 \u06c1\u06cc\u06ba raw data \u0641\u0631\u0627\u06c1\u0645 \u06a9\u0631 \u06a9\u06d2 \u062c\u0648 fused\u060c processed\u060c \u0627\u0648\u0631 decision-making \u06a9\u06d2 \u0644\u06cc\u06d2 \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06c1\u0648\u062a\u0627 \u06c1\u06d2\u06d4 ROS 2 \u06a9\u06d2 \u0633\u0627\u062a\u06be sensor integration \u06a9\u0648 \u0633\u0645\u062c\u06be\u0646\u0627 essential \u06c1\u06d2 functional humanoid robots \u0628\u0646\u0627\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2\u06d4"}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(n.p,{children:["\u0627\u0628 \u062c\u0628 \u06a9\u06c1 \u0622\u067e sensor integration \u06a9\u0648 \u0633\u0645\u062c\u06be \u06af\u0626\u06d2 \u06c1\u06cc\u06ba\u060c ",(0,o.jsx)(n.a,{href:"/ur/modules/module-2-digital-twins-simulation/humanoid-applications",children:"Humanoid Applications"})," \u067e\u0631 \u062c\u0627\u0626\u06cc\u06ba \u062a\u0627\u06a9\u06c1 \u0633\u06cc\u06a9\u06be\u06cc\u06ba \u06a9\u06c1 digital twins \u06a9\u06cc\u0633\u06d2 apply \u06c1\u0648\u062a\u06d2 \u06c1\u06cc\u06ba practice \u0645\u06cc\u06ba humanoid robotics development \u06a9\u06d2 \u0644\u06cc\u06d2\u06d4"]}),"\n",(0,o.jsx)(n.h2,{id:"related-content",children:"Related Content"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.a,{href:"/ur/modules/module-1-ros2-nervous-system/communication-patterns#publish-subscribe-pattern-topics",children:"\u0645\u0627\u0688\u06cc\u0648\u0644 1: ROS 2 Topics"})})," - Publish-subscribe pattern \u06a9\u06cc \u062a\u0641\u0635\u06cc\u0644\u06cc \u0648\u0636\u0627\u062d\u062a \u062c\u0648 sensor data \u06a9\u06d2 \u0644\u06cc\u06d2 \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06c1\u0648\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:(0,o.jsx)(n.a,{href:"/ur/modules/module-1-ros2-nervous-system/ros2-fundamentals#nodes",children:"\u0645\u0627\u0688\u06cc\u0648\u0644 1: ROS 2 Nodes"})})," - \u0633\u0645\u062c\u06be\u0646\u0627 \u06a9\u06c1 sensor nodes \u06a9\u06cc\u0633\u06d2 \u06a9\u0627\u0645 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba"]}),"\n"]})]})}function m(e={}){const{wrapper:n}={...(0,r.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(d,{...e})}):d(e)}},8453:(e,n,s)=>{s.d(n,{R:()=>t,x:()=>a});var i=s(6540);const o={},r=i.createContext(o);function t(e){const n=i.useContext(r);return i.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:t(e.components),i.createElement(r.Provider,{value:n},e.children)}}}]);