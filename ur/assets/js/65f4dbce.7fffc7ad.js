"use strict";(globalThis.webpackChunk=globalThis.webpackChunk||[]).push([[1848],{8453:(e,n,i)=>{i.d(n,{R:()=>r,x:()=>a});var t=i(6540);const o={},s=t.createContext(o);function r(e){const n=t.useContext(s);return t.useMemo(function(){return"function"==typeof e?e(n):{...n,...e}},[n,e])}function a(e){let n;return n=e.disableParentContext?"function"==typeof e.components?e.components(o):e.components||o:r(e.components),t.createElement(s.Provider,{value:n},e.children)}},9564:(e,n,i)=>{i.r(n),i.d(n,{assets:()=>c,contentTitle:()=>a,default:()=>d,frontMatter:()=>r,metadata:()=>t,toc:()=>l});const t=JSON.parse('{"id":"modules/module-4-vision-language-action/voice-to-action","title":"OpenAI Whisper \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2 Voice-to-Action","description":"\u0633\u0645\u062c\u06be\u0646\u0627 \u06a9\u06c1 OpenAI Whisper \u06a9\u06cc\u0633\u06d2 voice-to-action capabilities enable \u06a9\u0631\u062a\u0627 \u06c1\u06d2 humanoid robots \u06a9\u06d2 \u0644\u06cc\u06d2\u060c audio capture \u0633\u06d2 action generation \u062a\u06a9 complete pipeline \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/modules/module-4-vision-language-action/voice-to-action.md","sourceDirName":"modules/module-4-vision-language-action","slug":"/modules/module-4-vision-language-action/voice-to-action","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-4-vision-language-action/voice-to-action","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"vla","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/vla"},{"inline":true,"label":"voice-to-action","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/voice-to-action"},{"inline":true,"label":"whisper","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/whisper"},{"inline":true,"label":"speech-recognition","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/speech-recognition"},{"inline":true,"label":"natural-language","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/natural-language"},{"inline":true,"label":"humanoid-robotics","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/humanoid-robotics"}],"version":"current","sidebarPosition":3,"frontMatter":{"id":"voice-to-action","title":"OpenAI Whisper \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2 Voice-to-Action","sidebar_position":3,"description":"\u0633\u0645\u062c\u06be\u0646\u0627 \u06a9\u06c1 OpenAI Whisper \u06a9\u06cc\u0633\u06d2 voice-to-action capabilities enable \u06a9\u0631\u062a\u0627 \u06c1\u06d2 humanoid robots \u06a9\u06d2 \u0644\u06cc\u06d2\u060c audio capture \u0633\u06d2 action generation \u062a\u06a9 complete pipeline \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4","tags":["vla","voice-to-action","whisper","speech-recognition","natural-language","humanoid-robotics"],"learning_objectives":["lo-011"]},"sidebar":"textbookSidebar","previous":{"title":"LLM-Robotics Convergence","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-4-vision-language-action/llm-robotics-convergence"},"next":{"title":"LLMs \u06a9\u06d2 \u0633\u0627\u062a\u06be Cognitive Planning","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-4-vision-language-action/cognitive-planning"}}');var o=i(4848),s=i(8453);const r={id:"voice-to-action",title:"OpenAI Whisper \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2 Voice-to-Action",sidebar_position:3,description:"\u0633\u0645\u062c\u06be\u0646\u0627 \u06a9\u06c1 OpenAI Whisper \u06a9\u06cc\u0633\u06d2 voice-to-action capabilities enable \u06a9\u0631\u062a\u0627 \u06c1\u06d2 humanoid robots \u06a9\u06d2 \u0644\u06cc\u06d2\u060c audio capture \u0633\u06d2 action generation \u062a\u06a9 complete pipeline \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4",tags:["vla","voice-to-action","whisper","speech-recognition","natural-language","humanoid-robotics"],learning_objectives:["lo-011"]},a="OpenAI Whisper \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2 Voice-to-Action",c={},l=[{value:"OpenAI Whisper \u06a9\u06cc\u0633\u06d2 Voice-to-Action Enable \u06a9\u0631\u062a\u0627 \u06c1\u06d2",id:"openai-whisper-\u06a9\u06cc\u0633\u06d2-voice-to-action-enable-\u06a9\u0631\u062a\u0627-\u06c1\u06d2",level:2},{value:"VLA Systems \u0645\u06cc\u06ba Whisper \u06a9\u0627 Role",id:"vla-systems-\u0645\u06cc\u06ba-whisper-\u06a9\u0627-role",level:3},{value:"Voice-to-Action Pipeline",id:"voice-to-action-pipeline",level:2},{value:"Stage 1: Audio Capture",id:"stage-1-audio-capture",level:3},{value:"Stage 2: Speech Recognition",id:"stage-2-speech-recognition",level:3},{value:"Stage 3: Text Transcription",id:"stage-3-text-transcription",level:3},{value:"Stage 4: Action Generation",id:"stage-4-action-generation",level:3},{value:"Python Code Example: Whisper Integration Pattern",id:"python-code-example-whisper-integration-pattern",level:2},{value:"Key Integration Points",id:"key-integration-points",level:3},{value:"Voice-to-Action \u06a9\u0648 Cognitive Planning \u0633\u06d2 Connect \u06a9\u0631\u0646\u0627",id:"voice-to-action-\u06a9\u0648-cognitive-planning-\u0633\u06d2-connect-\u06a9\u0631\u0646\u0627",level:2},{value:"Voice-to-Action Pipeline Diagram",id:"voice-to-action-pipeline-diagram",level:2},{value:"Summary",id:"summary",level:2},{value:"Next Steps",id:"next-steps",level:2}];function p(e){const n={a:"a",code:"code",h1:"h1",h2:"h2",h3:"h3",header:"header",li:"li",ol:"ol",p:"p",pre:"pre",strong:"strong",ul:"ul",...(0,s.R)(),...e.components};return(0,o.jsxs)(o.Fragment,{children:[(0,o.jsx)(n.header,{children:(0,o.jsx)(n.h1,{id:"openai-whisper-\u0627\u0633\u062a\u0639\u0645\u0627\u0644-\u06a9\u0631\u062a\u06d2-\u06c1\u0648\u0626\u06d2-voice-to-action",children:"OpenAI Whisper \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2 Voice-to-Action"})}),"\n",(0,o.jsx)(n.p,{children:"Voice-to-action systems humans \u06a9\u0648 enable \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba robots \u06a9\u0648 control \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 natural speech \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2\u060c robot interaction \u06a9\u0648 intuitive \u0628\u0646\u0627\u062a\u06d2 \u06c1\u0648\u0626\u06d2 \u062c\u06cc\u0633\u06d2 \u06a9\u0633\u06cc \u062f\u0648\u0633\u0631\u06d2 person \u0633\u06d2 \u0628\u0627\u062a \u06a9\u0631\u0646\u0627\u06d4 \u06cc\u06c1 section explore \u06a9\u0631\u062a\u0627 \u06c1\u06d2 \u06a9\u06c1 OpenAI Whisper \u06a9\u06cc\u0633\u06d2 \u06cc\u06c1 capability enable \u06a9\u0631\u062a\u0627 \u06c1\u06d2 \u0627\u0648\u0631 complete pipeline voice input \u0633\u06d2 robot action \u062a\u06a9\u06d4"}),"\n",(0,o.jsx)(n.h2,{id:"openai-whisper-\u06a9\u06cc\u0633\u06d2-voice-to-action-enable-\u06a9\u0631\u062a\u0627-\u06c1\u06d2",children:"OpenAI Whisper \u06a9\u06cc\u0633\u06d2 Voice-to-Action Enable \u06a9\u0631\u062a\u0627 \u06c1\u06d2"}),"\n",(0,o.jsxs)(n.p,{children:[(0,o.jsx)(n.strong,{children:"OpenAI Whisper"})," \u0627\u06cc\u06a9 state-of-the-art speech recognition system \u06c1\u06d2 \u062c\u0648 spoken language \u06a9\u0648 text \u0645\u06cc\u06ba convert \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u06d4 Humanoid robots \u06a9\u06d2 \u0644\u06cc\u06d2\u060c Whisper bridge \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631 \u06a9\u0627\u0645 \u06a9\u0631\u062a\u0627 \u06c1\u06d2 human speech \u0627\u0648\u0631 robot understanding \u06a9\u06d2 \u062f\u0631\u0645\u06cc\u0627\u0646\u060c enable \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2:"]}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"High-accuracy transcription"}),": Speech \u06a9\u0648 text \u0645\u06cc\u06ba convert \u06a9\u0631\u0646\u0627 remarkable accuracy \u06a9\u06d2 \u0633\u0627\u062a\u06be languages \u0627\u0648\u0631 accents \u0645\u06cc\u06ba"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Real-time processing"}),": Audio streams process \u06a9\u0631\u0646\u0627 quickly enough interactive robot control \u06a9\u06d2 \u0644\u06cc\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Robust performance"}),": Noisy environments \u0627\u0648\u0631 varying audio quality handle \u06a9\u0631\u0646\u0627"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Language flexibility"}),": Multiple languages \u0627\u0648\u0631 dialects support \u06a9\u0631\u0646\u0627"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"vla-systems-\u0645\u06cc\u06ba-whisper-\u06a9\u0627-role",children:"VLA Systems \u0645\u06cc\u06ba Whisper \u06a9\u0627 Role"}),"\n",(0,o.jsx)(n.p,{children:"VLA systems \u0645\u06cc\u06ba\u060c Whisper critical first step perform \u06a9\u0631\u062a\u0627 \u06c1\u06d2 audio input \u06a9\u0648 text \u0645\u06cc\u06ba convert \u06a9\u0631\u0646\u06d2 \u06a9\u0627 \u062c\u0648 language models process \u06a9\u0631 \u0633\u06a9\u062a\u06d2 \u06c1\u06cc\u06ba:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Audio capture"}),": Microphones spoken commands capture \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Speech recognition"}),": Whisper audio \u06a9\u0648 text \u0645\u06cc\u06ba transcribe \u06a9\u0631\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Text processing"}),": Transcribed text cognitive planning \u06a9\u06d2 \u0644\u06cc\u06d2 input \u0628\u0646 \u062c\u0627\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action generation"}),": Cognitive planning text \u06a9\u0648 robot actions \u0645\u06cc\u06ba translate \u06a9\u0631\u062a\u06cc \u06c1\u06d2"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"voice-to-action-pipeline",children:"Voice-to-Action Pipeline"}),"\n",(0,o.jsx)(n.p,{children:"Complete voice-to-action pipeline \u0686\u0627\u0631 main stages \u067e\u0631 \u0645\u0634\u062a\u0645\u0644 \u06c1\u06d2:"}),"\n",(0,o.jsx)(n.h3,{id:"stage-1-audio-capture",children:"Stage 1: Audio Capture"}),"\n",(0,o.jsx)(n.p,{children:"Robot \u06a9\u06cc microphones spoken commands capture \u06a9\u0631\u062a\u06cc \u06c1\u06cc\u06ba audio waveforms \u06a9\u06d2 \u0637\u0648\u0631 \u067e\u0631\u06d4 \u06cc\u06c1 stage \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u0627 \u06c1\u06d2:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Audio acquisition"}),": Microphones \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 sound waves capture \u06a9\u0631\u0646\u0627"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Preprocessing"}),": Noise filter \u06a9\u0631\u0646\u0627 \u0627\u0648\u0631 audio levels normalize \u06a9\u0631\u0646\u0627"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Format conversion"}),": Audio prepare \u06a9\u0631\u0646\u0627 format \u0645\u06cc\u06ba suitable speech recognition \u06a9\u06d2 \u0644\u06cc\u06d2"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"stage-2-speech-recognition",children:"Stage 2: Speech Recognition"}),"\n",(0,o.jsx)(n.p,{children:"OpenAI Whisper audio process \u06a9\u0631\u062a\u0627 \u06c1\u06d2 \u0627\u0648\u0631 \u0627\u0633\u06d2 text \u0645\u06cc\u06ba convert \u06a9\u0631\u062a\u0627 \u06c1\u06d2:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Audio analysis"}),": Speech patterns \u06a9\u06d2 \u0644\u06cc\u06d2 audio waveform analyze \u06a9\u0631\u0646\u0627"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Language detection"}),": Language identify \u06a9\u0631\u0646\u0627 \u062c\u0648 spoken \u06c1\u0648 \u0631\u06c1\u06cc \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Text transcription"}),": Speech \u06a9\u0648 written text \u0645\u06cc\u06ba convert \u06a9\u0631\u0646\u0627"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Confidence scoring"}),": Transcription accuracy \u06a9\u06d2 \u0644\u06cc\u06d2 confidence levels \u0641\u0631\u0627\u06c1\u0645 \u06a9\u0631\u0646\u0627"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"stage-3-text-transcription",children:"Stage 3: Text Transcription"}),"\n",(0,o.jsx)(n.p,{children:"Transcribed text further processing \u06a9\u06d2 \u0644\u06cc\u06d2 prepare \u06a9\u06cc \u062c\u0627\u062a\u06cc \u06c1\u06d2:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Text normalization"}),": Formatting \u0627\u0648\u0631 punctuation standardize \u06a9\u0631\u0646\u0627"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Intent extraction"}),": Core command \u06cc\u0627 request identify \u06a9\u0631\u0646\u0627"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Context preservation"}),": Conversation \u0633\u06d2 relevant context \u0628\u0631\u0642\u0631\u0627\u0631 \u0631\u06a9\u06be\u0646\u0627"]}),"\n"]}),"\n",(0,o.jsx)(n.h3,{id:"stage-4-action-generation",children:"Stage 4: Action Generation"}),"\n",(0,o.jsx)(n.p,{children:"Transcribed text cognitive planning systems \u06a9\u0648 pass \u06a9\u06cc \u062c\u0627\u062a\u06cc \u06c1\u06d2:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Intent understanding"}),": Determine \u06a9\u0631\u0646\u0627 \u06a9\u06c1 user \u06a9\u06cc\u0627 \u0686\u0627\u06c1\u062a\u0627 \u06c1\u06d2 robot \u06a9\u0631\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Plan generation"}),": Robot actions \u06a9\u06cc sequence create \u06a9\u0631\u0646\u0627"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Action execution"}),": ROS 2 \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 generated actions execute \u06a9\u0631\u0646\u0627"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"python-code-example-whisper-integration-pattern",children:"Python Code Example: Whisper Integration Pattern"}),"\n",(0,o.jsx)(n.p,{children:"\u062f\u0631\u062c \u0630\u06cc\u0644 example demonstrate \u06a9\u0631\u062a\u0627 \u06c1\u06d2 \u06a9\u06c1 Whisper \u06a9\u06cc\u0633\u06d2 integrate \u06c1\u0648 \u0633\u06a9\u062a\u0627 \u06c1\u06d2 voice-to-action system \u0645\u06cc\u06ba\u06d4 \u06cc\u06c1 conceptual pattern \u06c1\u06d2 key integration points \u062f\u06a9\u06be\u0627\u062a\u06d2 \u06c1\u0648\u0626\u06d2:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-python",children:'# Voice-to-Action Integration Pattern\n# This example shows how Whisper API calls integrate with VLA systems\n\nimport whisper\nimport rclpy\nfrom rclpy.node import Node\nfrom std_msgs.msg import String\n\nclass VoiceToActionNode(Node):\n    """\n    ROS 2 node that integrates Whisper for voice-to-action capabilities.\n    This demonstrates the integration pattern, not a full implementation.\n    """\n    \n    def __init__(self):\n        super().__init__(\'voice_to_action_node\')\n        \n        # Initialize Whisper model (conceptual - actual initialization varies)\n        # In practice, you would load the Whisper model or connect to API\n        self.whisper_model = whisper.load_model("base")  # Example model loading\n        \n        # ROS 2 publisher for sending transcribed text to cognitive planning\n        self.text_publisher = self.create_publisher(\n            String, \n            \'voice_transcription\', \n            10\n        )\n        \n        # ROS 2 subscriber for receiving audio data\n        self.audio_subscriber = self.create_subscription(\n            AudioData,  # Audio message type (conceptual)\n            \'audio_input\',\n            self.process_audio_callback,\n            10\n        )\n    \n    def process_audio_callback(self, audio_msg):\n        """\n        Process incoming audio and transcribe using Whisper.\n        This demonstrates the voice-to-action pipeline integration.\n        """\n        # Stage 1: Audio capture (already received via ROS 2)\n        audio_data = audio_msg.data\n        \n        # Stage 2: Speech recognition using Whisper\n        # Whisper transcribes audio to text\n        result = self.whisper_model.transcribe(audio_data)\n        transcribed_text = result["text"]\n        \n        # Stage 3: Text transcription (normalization)\n        normalized_text = self.normalize_text(transcribed_text)\n        \n        # Stage 4: Publish to cognitive planning system\n        # The cognitive planning node will receive this and generate actions\n        text_msg = String()\n        text_msg.data = normalized_text\n        self.text_publisher.publish(text_msg)\n        \n        self.get_logger().info(f\'Transcribed: {transcribed_text}\')\n    \n    def normalize_text(self, text):\n        """\n        Normalize transcribed text for cognitive planning.\n        This is a simplified example of text preprocessing.\n        """\n        # Remove extra whitespace, normalize capitalization, etc.\n        normalized = text.strip().lower()\n        return normalized\n\ndef main():\n    rclpy.init()\n    node = VoiceToActionNode()\n    rclpy.spin(node)\n    rclpy.shutdown()\n\nif __name__ == \'__main__\':\n    main()\n'})}),"\n",(0,o.jsx)(n.h3,{id:"key-integration-points",children:"Key Integration Points"}),"\n",(0,o.jsx)(n.p,{children:"\u06cc\u06c1 example demonstrate \u06a9\u0631\u062a\u0627 \u06c1\u06d2:"}),"\n",(0,o.jsxs)(n.ol,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Whisper model loading"}),": Whisper \u06a9\u06cc\u0633\u06d2 initialized \u06c1\u0648\u062a\u0627 \u06c1\u06d2 (conceptual pattern)"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Audio processing"}),": Audio \u06a9\u06cc\u0633\u06d2 system \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 flow \u06a9\u0631\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Text transcription"}),": Whisper \u06a9\u06cc\u0633\u06d2 speech \u06a9\u0648 text \u0645\u06cc\u06ba convert \u06a9\u0631\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"ROS 2 integration"}),": Transcribed text \u06a9\u06cc\u0633\u06d2 cognitive planning \u06a9\u06d2 \u0644\u06cc\u06d2 published \u06c1\u0648\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Pipeline flow"}),": \u062a\u0645\u0627\u0645 stages \u06a9\u06cc\u0633\u06d2 connect \u06c1\u0648\u062a\u06d2 \u06c1\u06cc\u06ba voice-to-action pipeline \u0645\u06cc\u06ba"]}),"\n"]}),"\n",(0,o.jsx)(n.h2,{id:"voice-to-action-\u06a9\u0648-cognitive-planning-\u0633\u06d2-connect-\u06a9\u0631\u0646\u0627",children:"Voice-to-Action \u06a9\u0648 Cognitive Planning \u0633\u06d2 Connect \u06a9\u0631\u0646\u0627"}),"\n",(0,o.jsx)(n.p,{children:"Voice-to-action pipeline seamlessly cognitive planning \u0633\u06d2 connect \u06c1\u0648\u062a\u0627 \u06c1\u06d2:"}),"\n",(0,o.jsxs)(n.ul,{children:["\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Text output"}),": Whisper \u06a9\u06cc transcribed text cognitive planning \u06a9\u06d2 \u0644\u06cc\u06d2 input \u0628\u0646 \u062c\u0627\u062a\u06cc \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Intent preservation"}),": Spoken commands \u06a9\u0627 meaning \u0627\u0648\u0631 intent text \u0645\u06cc\u06ba preserved \u06c1\u0648\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Context continuity"}),": Conversation context multiple voice commands \u0645\u06cc\u06ba \u0628\u0631\u0642\u0631\u0627\u0631 \u0631\u06a9\u06be\u0627 \u062c\u0627 \u0633\u06a9\u062a\u0627 \u06c1\u06d2"]}),"\n",(0,o.jsxs)(n.li,{children:[(0,o.jsx)(n.strong,{children:"Error handling"}),": Transcription errors cognitive planning \u0633\u06d2 \u067e\u06c1\u0644\u06d2 detect \u0627\u0648\u0631 handle \u06a9\u06cc\u06d2 \u062c\u0627 \u0633\u06a9\u062a\u06d2 \u06c1\u06cc\u06ba"]}),"\n"]}),"\n",(0,o.jsxs)(n.p,{children:["\u06cc\u06c1 connection complete flow enable \u06a9\u0631\u062a\u0627 \u06c1\u06d2: ",(0,o.jsx)(n.strong,{children:"Voice \u2192 Text \u2192 Cognitive Plan \u2192 Robot Actions"}),"\u06d4"]}),"\n",(0,o.jsx)(n.h2,{id:"voice-to-action-pipeline-diagram",children:"Voice-to-Action Pipeline Diagram"}),"\n",(0,o.jsx)(n.p,{children:"\u062f\u0631\u062c \u0630\u06cc\u0644 diagram complete voice-to-action pipeline illustrate \u06a9\u0631\u062a\u0627 \u06c1\u06d2:"}),"\n",(0,o.jsx)(n.pre,{children:(0,o.jsx)(n.code,{className:"language-mermaid",children:"flowchart TD\n    A[Audio Capture] --\x3e B[Microphone Input]\n    B --\x3e C[Audio Preprocessing]\n    C --\x3e D[OpenAI Whisper]\n    D --\x3e E[Speech Recognition]\n    E --\x3e F[Text Transcription]\n    F --\x3e G[Text Normalization]\n    G --\x3e H[Cognitive Planning System]\n    H --\x3e I[Action Generation]\n    I --\x3e J[ROS 2 Actions]\n    J --\x3e K[Robot Execution]\n    \n    style A fill:#e1f5ff\n    style D fill:#fff4e1\n    style H fill:#e8f5e9\n    style K fill:#fce4ec\n"})}),"\n",(0,o.jsx)(n.p,{children:"\u06cc\u06c1 diagram \u062f\u06a9\u06be\u0627\u062a\u0627 \u06c1\u06d2 \u06a9\u06c1 audio \u06a9\u06cc\u0633\u06d2 system \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 flow \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c capture \u0633\u06d2 robot execution \u062a\u06a9\u060c Whisper critical speech-to-text conversion perform \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4"}),"\n",(0,o.jsx)(n.h2,{id:"summary",children:"Summary"}),"\n",(0,o.jsx)(n.p,{children:"OpenAI Whisper voice-to-action capabilities enable \u06a9\u0631\u062a\u0627 \u06c1\u06d2 spoken commands \u06a9\u0648 text \u0645\u06cc\u06ba convert \u06a9\u0631 \u06a9\u06d2 \u062c\u0648 cognitive planning systems process \u06a9\u0631 \u0633\u06a9\u062a\u06d2 \u06c1\u06cc\u06ba\u06d4 Complete pipeline flow \u06a9\u0631\u062a\u0627 \u06c1\u06d2 audio capture \u0633\u06d2 speech recognition\u060c text transcription\u060c \u0627\u0648\u0631 action generation \u062a\u06a9\u06d4 \u06cc\u06c1 pipeline \u0633\u0645\u062c\u06be\u0646\u0627 essential \u06c1\u06d2 comprehending \u06a9\u06d2 \u0644\u06cc\u06d2 \u06a9\u06c1 natural language input \u06a9\u06cc\u0633\u06d2 robot behavior \u0628\u0646 \u062c\u0627\u062a\u0627 \u06c1\u06d2\u06d4"}),"\n",(0,o.jsx)(n.h2,{id:"next-steps",children:"Next Steps"}),"\n",(0,o.jsxs)(n.p,{children:["\u0627\u0628 \u062c\u0628 \u06a9\u06c1 \u0622\u067e voice-to-action systems \u0633\u0645\u062c\u06be \u06af\u0626\u06d2 \u06c1\u06cc\u06ba\u060c ",(0,o.jsx)(n.a,{href:"/ur/modules/module-4-vision-language-action/cognitive-planning",children:"Cognitive Planning"})," \u067e\u0631 \u062c\u0627\u0626\u06cc\u06ba \u0633\u06cc\u06a9\u06be\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 \u06a9\u06c1 LLMs \u06a9\u06cc\u0633\u06d2 natural language commands translate \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba ROS 2 action sequences \u0645\u06cc\u06ba\u06d4"]})]})}function d(e={}){const{wrapper:n}={...(0,s.R)(),...e.components};return n?(0,o.jsx)(n,{...e,children:(0,o.jsx)(p,{...e})}):p(e)}}}]);