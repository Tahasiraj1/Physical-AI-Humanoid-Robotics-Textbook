"use strict";(globalThis.webpackChunk=globalThis.webpackChunk||[]).push([[4570],{8453:(n,e,i)=>{i.d(e,{R:()=>a,x:()=>c});var o=i(6540);const t={},s=o.createContext(t);function a(n){const e=o.useContext(s);return o.useMemo(function(){return"function"==typeof n?n(e):{...e,...n}},[e,n])}function c(n){let e;return e=n.disableParentContext?"function"==typeof n.components?n.components(t):n.components||t:a(n.components),o.createElement(s.Provider,{value:e},n.children)}},9903:(n,e,i)=>{i.r(e),i.d(e,{assets:()=>r,contentTitle:()=>c,default:()=>g,frontMatter:()=>a,metadata:()=>o,toc:()=>l});const o=JSON.parse('{"id":"modules/module-4-vision-language-action/glossary","title":"Glossary - Key Terminology","description":"\u0645\u0627\u0688\u06cc\u0648\u0644 4: Vision-Language-Action (VLA) \u06a9\u06d2 \u0644\u06cc\u06d2 \u06a9\u0644\u06cc\u062f\u06cc \u0627\u0635\u0637\u0644\u0627\u062d\u0627\u062a \u06a9\u06cc \u062a\u0639\u0631\u06cc\u0641\u06cc\u06ba\u060c VLA\u060c voice-to-action\u060c cognitive planning\u060c \u0627\u0648\u0631 related concepts \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4","source":"@site/i18n/ur/docusaurus-plugin-content-docs/current/modules/module-4-vision-language-action/glossary.md","sourceDirName":"modules/module-4-vision-language-action","slug":"/modules/module-4-vision-language-action/glossary","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-4-vision-language-action/glossary","draft":false,"unlisted":false,"tags":[{"inline":true,"label":"vla","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/vla"},{"inline":true,"label":"glossary","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/glossary"},{"inline":true,"label":"terminology","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/terminology"},{"inline":true,"label":"definitions","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/definitions"}],"version":"current","sidebarPosition":8,"frontMatter":{"id":"glossary","title":"Glossary - Key Terminology","sidebar_position":8,"description":"\u0645\u0627\u0688\u06cc\u0648\u0644 4: Vision-Language-Action (VLA) \u06a9\u06d2 \u0644\u06cc\u06d2 \u06a9\u0644\u06cc\u062f\u06cc \u0627\u0635\u0637\u0644\u0627\u062d\u0627\u062a \u06a9\u06cc \u062a\u0639\u0631\u06cc\u0641\u06cc\u06ba\u060c VLA\u060c voice-to-action\u060c cognitive planning\u060c \u0627\u0648\u0631 related concepts \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4","tags":["vla","glossary","terminology","definitions"]},"sidebar":"textbookSidebar","previous":{"title":"Module Integration - VLA \u06a9\u0648 Previous Modules \u0633\u06d2 Connect \u06a9\u0631\u0646\u0627","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-4-vision-language-action/module-integration"}}');var t=i(4848),s=i(8453);const a={id:"glossary",title:"Glossary - Key Terminology",sidebar_position:8,description:"\u0645\u0627\u0688\u06cc\u0648\u0644 4: Vision-Language-Action (VLA) \u06a9\u06d2 \u0644\u06cc\u06d2 \u06a9\u0644\u06cc\u062f\u06cc \u0627\u0635\u0637\u0644\u0627\u062d\u0627\u062a \u06a9\u06cc \u062a\u0639\u0631\u06cc\u0641\u06cc\u06ba\u060c VLA\u060c voice-to-action\u060c cognitive planning\u060c \u0627\u0648\u0631 related concepts \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4",tags:["vla","glossary","terminology","definitions"]},c="Glossary: Key Terminology",r={},l=[{value:"Vision-Language-Action (VLA)",id:"vision-language-action-vla",level:2},{value:"Voice-to-Action",id:"voice-to-action",level:2},{value:"Cognitive Planning",id:"cognitive-planning",level:2},{value:"Natural Language Intent",id:"natural-language-intent",level:2},{value:"Action Sequence",id:"action-sequence",level:2},{value:"VLA Pipeline",id:"vla-pipeline",level:2},{value:"Cognitive Plan",id:"cognitive-plan",level:2},{value:"Voice Command",id:"voice-command",level:2},{value:"Action Sequence",id:"action-sequence-1",level:2},{value:"Capstone Project Scenario",id:"capstone-project-scenario",level:2}];function d(n){const e={h1:"h1",h2:"h2",header:"header",p:"p",strong:"strong",...(0,s.R)(),...n.components};return(0,t.jsxs)(t.Fragment,{children:[(0,t.jsx)(e.header,{children:(0,t.jsx)(e.h1,{id:"glossary-key-terminology",children:"Glossary: Key Terminology"})}),"\n",(0,t.jsx)(e.p,{children:"\u06cc\u06c1 glossary \u0645\u0627\u0688\u06cc\u0648\u0644 4: Vision-Language-Action (VLA) \u0645\u06cc\u06ba \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06c1\u0648\u0646\u06d2 \u0648\u0627\u0644\u06cc \u06a9\u0644\u06cc\u062f\u06cc \u0627\u0635\u0637\u0644\u0627\u062d\u0627\u062a \u06a9\u06cc \u062a\u0639\u0631\u06cc\u0641 \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u06d4"}),"\n",(0,t.jsx)(e.h2,{id:"vision-language-action-vla",children:"Vision-Language-Action (VLA)"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Definition"}),": \u0627\u06cc\u06a9 unified framework \u062c\u0648 vision\u060c language processing\u060c \u0627\u0648\u0631 action execution combine \u06a9\u0631\u062a\u0627 \u06c1\u06d2 natural language interaction enable \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 robots \u06a9\u06d2 \u0633\u0627\u062a\u06be\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Context"}),": VLA systems robots \u06a9\u0648 enable \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba spoken \u06cc\u0627 written commands \u0633\u0645\u062c\u06be\u0646\u06d2\u060c \u0627\u067e\u0646\u06d2 environment perceive \u06a9\u0631\u0646\u06d2\u060c \u0627\u0648\u0631 language instructions \u06a9\u06cc \u0628\u0646\u06cc\u0627\u062f \u067e\u0631 physical actions execute \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Related Terms"}),": Cognitive planning\u060c voice-to-action\u060c natural language processing"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Example"}),': \u0627\u06cc\u06a9 VLA system user \u06a9\u0648 allow \u06a9\u0631\u062a\u0627 \u06c1\u06d2 \u06a9\u06c1 "\u0633\u0631\u062e \u06a9\u067e \u0627\u0679\u06be\u0627\u0624" \u06a9\u06c1\u06d2 \u0627\u0648\u0631 robot command \u0633\u0645\u062c\u06be\u062a\u0627 \u06c1\u06d2\u060c cup identify \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c \u0627\u0648\u0631 \u0627\u0633\u06d2 \u0627\u0679\u06be\u0627\u062a\u0627 \u06c1\u06d2\u06d4']}),"\n",(0,t.jsx)(e.h2,{id:"voice-to-action",children:"Voice-to-Action"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Definition"}),": Capability \u062c\u0648 robots \u06a9\u0648 enable \u06a9\u0631\u062a\u06cc \u06c1\u06d2 spoken commands \u06a9\u0648 robot actions \u0645\u06cc\u06ba convert \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 speech recognition \u0627\u0648\u0631 cognitive planning \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Context"}),": Voice-to-action systems speech recognition \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba (\u062c\u06cc\u0633\u06d2 OpenAI Whisper) spoken commands transcribe \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2\u060c \u062c\u0648 \u067e\u06be\u0631 cognitive planning systems \u06a9\u06cc \u0637\u0631\u0641 \u0633\u06d2 process \u06c1\u0648\u062a\u06d2 \u06c1\u06cc\u06ba robot actions generate \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Related Terms"}),": Speech recognition\u060c cognitive planning\u060c natural language processing"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Example"}),': \u062c\u0628 user "\u06a9\u0645\u0631\u06c1 \u0635\u0627\u0641 \u06a9\u0631\u0648" \u06a9\u06c1\u062a\u0627 \u06c1\u06d2\u060c voice-to-action system speech transcribe \u06a9\u0631\u062a\u0627 \u06c1\u06d2 \u0627\u0648\u0631 task accomplish \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 robot actions \u06a9\u06cc sequence generate \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u06d4']}),"\n",(0,t.jsx)(e.h2,{id:"cognitive-planning",children:"Cognitive Planning"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Definition"}),": Process \u062c\u0633 \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2 Large Language Models (LLMs) natural language commands translate \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba executable robot actions \u06a9\u06cc sequences \u0645\u06cc\u06ba\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Context"}),": Cognitive planning high-level natural language instructions decompose \u06a9\u0631\u062a\u06cc \u06c1\u06d2 structured action plans \u0645\u06cc\u06ba \u062c\u0648 robots execute \u06a9\u0631 \u0633\u06a9\u062a\u06d2 \u06c1\u06cc\u06ba ROS 2 actions \u06a9\u06d2 \u0630\u0631\u06cc\u0639\u06d2\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Related Terms"}),": LLM\u060c natural language processing\u060c action sequence\u060c ROS 2"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Example"}),': Cognitive planning "\u06a9\u0645\u0631\u06c1 \u0635\u0627\u0641 \u06a9\u0631\u0648" \u06a9\u0648 navigation\u060c perception\u060c \u0627\u0648\u0631 manipulation actions \u06a9\u06cc sequence \u0645\u06cc\u06ba translate \u06a9\u0631\u062a\u06cc \u06c1\u06d2\u06d4']}),"\n",(0,t.jsx)(e.h2,{id:"natural-language-intent",children:"Natural Language Intent"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Definition"}),": Semantic meaning \u0627\u0648\u0631 goal extracted voice command \u06cc\u0627 text instruction \u0633\u06d2\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Context"}),": Natural language intent represent \u06a9\u0631\u062a\u0627 \u06c1\u06d2 \u06a9\u06c1 user \u06a9\u06cc\u0627 \u0686\u0627\u06c1\u062a\u0627 \u06c1\u06d2 robot accomplish \u06a9\u0631\u06d2\u060c goal\u060c required capabilities\u060c constraints\u060c \u0627\u0648\u0631 context \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Related Terms"}),": Intent understanding\u060c goal decomposition\u060c cognitive planning"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Example"}),': "\u0633\u0631\u062e \u06a9\u067e \u0627\u0679\u06be\u0627\u0624" \u06a9\u0627 natural language intent \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u0627 \u06c1\u06d2 goal (pick up)\u060c target (red cup)\u060c \u0627\u0648\u0631 implicit constraints (appropriate grasp \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u06cc\u06ba)\u06d4']}),"\n",(0,t.jsx)(e.h2,{id:"action-sequence",children:"Action Sequence"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Definition"}),": ROS 2 actions \u06a9\u06cc ordered list \u062c\u0648 cognitive plan implement \u06a9\u0631\u062a\u06cc \u06c1\u06d2\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Context"}),": Action sequences executable robot behaviors represent \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba cognitive planning \u0633\u06d2 generated\u060c individual actions\u060c action parameters\u060c dependencies\u060c \u0627\u0648\u0631 execution order \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Related Terms"}),": ROS 2 actions\u060c cognitive plan\u060c action execution"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Example"}),': "\u06a9\u067e \u0627\u0679\u06be\u0627\u0624" \u06a9\u06d2 \u0644\u06cc\u06d2 action sequence \u0634\u0627\u0645\u0644 \u06a9\u0631 \u0633\u06a9\u062a\u06cc \u06c1\u06d2: table \u067e\u0631 navigate\u060c objects detect\u060c cup identify\u060c grasp plan\u060c grasp execute\u06d4']}),"\n",(0,t.jsx)(e.h2,{id:"vla-pipeline",children:"VLA Pipeline"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Definition"}),": Complete flow voice input \u0633\u06d2 physical action \u062a\u06a9 VLA systems \u0645\u06cc\u06ba\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Context"}),": VLA pipeline \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u0627 \u06c1\u06d2 voice capture\u060c speech recognition\u060c text transcription\u060c cognitive planning\u060c action generation\u060c perception\u060c navigation\u060c \u0627\u0648\u0631 manipulation stages\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Related Terms"}),": Voice-to-action\u060c cognitive planning\u060c action execution"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Example"}),": VLA pipeline flow \u06a9\u0631\u062a\u0627 \u06c1\u06d2: Voice command \u2192 Whisper transcription \u2192 Cognitive planning \u2192 ROS 2 actions \u2192 Robot execution\u06d4"]}),"\n",(0,t.jsx)(e.h2,{id:"cognitive-plan",children:"Cognitive Plan"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Definition"}),": Robot actions \u06a9\u06cc structured sequence generated LLM \u0633\u06d2 natural language input \u0633\u06d2\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Context"}),": Cognitive plans bridge natural language intent \u0633\u06d2 executable robot behaviors \u062a\u06a9\u060c high-level goals\u060c decomposed sub-tasks\u060c action sequences\u060c \u0627\u0648\u0631 execution parameters contain \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Related Terms"}),": Cognitive planning\u060c action sequence\u060c natural language intent"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Example"}),': "\u06a9\u0645\u0631\u06c1 \u0635\u0627\u0641 \u06a9\u0631\u0648" \u06a9\u06d2 \u0644\u06cc\u06d2 cognitive plan \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u06cc \u06c1\u06d2 sub-tasks \u062c\u06cc\u0633\u06d2 navigate\u060c objects identify\u060c objects pick up\u060c \u0627\u0648\u0631 objects place\u060c \u06c1\u0631 \u0627\u06cc\u06a9 associated ROS 2 actions \u06a9\u06d2 \u0633\u0627\u062a\u06be\u06d4']}),"\n",(0,t.jsx)(e.h2,{id:"voice-command",children:"Voice Command"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Definition"}),": \u0627\u06cc\u06a9 spoken instruction \u062f\u06cc \u06af\u0626\u06cc humanoid robot \u06a9\u0648\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Context"}),": Voice commands contain \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba audio waveform data\u060c transcribed text\u060c semantic meaning\u060c \u0627\u0648\u0631 intent\u060c natural language human-robot interaction enable \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Related Terms"}),": Voice-to-action\u060c speech recognition\u060c natural language intent"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Example"}),': "table \u0633\u06d2 \u0633\u0631\u062e \u06a9\u067e \u0627\u0679\u06be\u0627\u0624" \u0627\u06cc\u06a9 voice command \u06c1\u06d2 \u062c\u0648 VLA pipeline initiate \u06a9\u0631\u062a\u06cc \u06c1\u06d2\u06d4']}),"\n",(0,t.jsx)(e.h2,{id:"action-sequence-1",children:"Action Sequence"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Definition"}),": ROS 2 actions \u06a9\u06cc ordered list \u062c\u0648 cognitive plan implement \u06a9\u0631\u062a\u06cc \u06c1\u06d2\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Context"}),": Action sequences contain \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba individual actions (navigation\u060c manipulation\u060c perception)\u060c action parameters\u060c actions \u06a9\u06d2 \u062f\u0631\u0645\u06cc\u0627\u0646 dependencies\u060c \u0627\u0648\u0631 execution order\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Related Terms"}),": ROS 2 actions\u060c cognitive plan\u060c action execution"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Example"}),": Action sequence \u0634\u0627\u0645\u0644 \u06a9\u0631 \u0633\u06a9\u062a\u06cc \u06c1\u06d2: NavigateToPose\u060c DetectObjects\u060c PickPlace actions specific order \u0645\u06cc\u06ba\u06d4"]}),"\n",(0,t.jsx)(e.h2,{id:"capstone-project-scenario",children:"Capstone Project Scenario"}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Definition"}),": \u0627\u06cc\u06a9 complete autonomous behavior demonstration integrating \u062a\u0645\u0627\u0645 VLA components\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Context"}),": Capstone project scenarios demonstrate \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba VLA concepts \u06a9\u06cc practical application\u060c voice command input\u060c planning phase\u060c navigation phase\u060c object identification phase\u060c \u0627\u0648\u0631 manipulation phase \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Related Terms"}),": VLA pipeline\u060c autonomous behavior\u060c integration"]}),"\n",(0,t.jsxs)(e.p,{children:[(0,t.jsx)(e.strong,{children:"Example"}),": Capstone project demonstrate \u06a9\u0631\u062a\u0627 \u06c1\u06d2 complete scenario \u062c\u06c1\u0627\u06ba robot voice command receive \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c path plan \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c obstacles navigate \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c objects identify \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c \u0627\u0648\u0631 \u0627\u0646\u06c1\u06cc\u06ba manipulate \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u06d4"]})]})}function g(n={}){const{wrapper:e}={...(0,s.R)(),...n.components};return e?(0,t.jsx)(e,{...n,children:(0,t.jsx)(d,{...n})}):d(n)}}}]);