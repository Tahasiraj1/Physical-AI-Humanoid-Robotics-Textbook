"use strict";(globalThis.webpackChunk=globalThis.webpackChunk||[]).push([[3914],{3765:o=>{o.exports=JSON.parse('{"tag":{"label":"vla","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags/vla","allTagsPath":"/Physical-AI-Humanoid-Robotics-Textbook/ur/tags","count":9,"items":[{"id":"modules/module-4-vision-language-action/capstone-project","title":"Capstone Project - The Autonomous Humanoid","description":"Complete VLA pipeline demonstration \u062f\u06a9\u06be\u0627\u062a\u06d2 \u06c1\u0648\u0626\u06d2 \u06a9\u06c1 simulated humanoid robot \u06a9\u06cc\u0633\u06d2 voice commands receive \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c paths plan \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c obstacles navigate \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c objects identify \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u060c \u0627\u0648\u0631 \u0627\u0646\u06c1\u06cc\u06ba manipulate \u06a9\u0631\u062a\u0627 \u06c1\u06d2\u06d4","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-4-vision-language-action/capstone-project"},{"id":"modules/module-4-vision-language-action/glossary","title":"Glossary - Key Terminology","description":"\u0645\u0627\u0688\u06cc\u0648\u0644 4: Vision-Language-Action (VLA) \u06a9\u06d2 \u0644\u06cc\u06d2 \u06a9\u0644\u06cc\u062f\u06cc \u0627\u0635\u0637\u0644\u0627\u062d\u0627\u062a \u06a9\u06cc \u062a\u0639\u0631\u06cc\u0641\u06cc\u06ba\u060c VLA\u060c voice-to-action\u060c cognitive planning\u060c \u0627\u0648\u0631 related concepts \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-4-vision-language-action/glossary"},{"id":"modules/module-4-vision-language-action/safety-validation","title":"LLM-Generated Plans \u06a9\u06cc Safety & Validation","description":"\u0633\u0645\u062c\u06be\u0646\u0627 \u06a9\u06c1 LLM-generated action plans \u06a9\u06cc\u0633\u06d2 validated \u0627\u0648\u0631 safely executed \u06c1\u0648\u062a\u06d2 \u06c1\u06cc\u06ba\u060c plan verification \u0627\u0648\u0631 constraint checking \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-4-vision-language-action/safety-validation"},{"id":"modules/module-4-vision-language-action/llm-robotics-convergence","title":"LLM-Robotics Convergence","description":"\u0633\u0645\u062c\u06be\u0646\u0627 \u06a9\u06c1 Large Language Models (LLMs) \u06a9\u06cc\u0633\u06d2 robotics \u06a9\u06d2 \u0633\u0627\u062a\u06be converge \u06c1\u0648\u062a\u06d2 \u06c1\u06cc\u06ba natural language interaction enable \u06a9\u0631\u0646\u06d2 \u06a9\u06d2 \u0644\u06cc\u06d2 humanoid robots \u06a9\u06d2 \u0633\u0627\u062a\u06be\u06d4","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-4-vision-language-action/llm-robotics-convergence"},{"id":"modules/module-4-vision-language-action/cognitive-planning","title":"LLMs \u06a9\u06d2 \u0633\u0627\u062a\u06be Cognitive Planning","description":"\u0633\u0645\u062c\u06be\u0646\u0627 \u06a9\u06c1 LLMs \u06a9\u06cc\u0633\u06d2 cognitive planning perform \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba natural language commands translate \u06a9\u0631 \u06a9\u06d2 ROS 2 action sequences \u0645\u06cc\u06ba\u06d4","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-4-vision-language-action/cognitive-planning"},{"id":"modules/module-4-vision-language-action/module-integration","title":"Module Integration - VLA \u06a9\u0648 Previous Modules \u0633\u06d2 Connect \u06a9\u0631\u0646\u0627","description":"\u0633\u0645\u062c\u06be\u0646\u0627 \u06a9\u06c1 VLA concepts \u06a9\u06cc\u0633\u06d2 connect \u06c1\u0648\u062a\u06d2 \u06c1\u06cc\u06ba \u0627\u0648\u0631 build \u06a9\u0631\u062a\u06d2 \u06c1\u06cc\u06ba concepts \u067e\u0631 \u0645\u0627\u0688\u06cc\u0648\u0644 1\u060c 2\u060c \u0627\u0648\u0631 3 \u0633\u06d2\u060c ROS 2 integration\u060c simulation support\u060c \u0627\u0648\u0631 perception integration \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-4-vision-language-action/module-integration"},{"id":"modules/module-4-vision-language-action/voice-to-action","title":"OpenAI Whisper \u0627\u0633\u062a\u0639\u0645\u0627\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2 Voice-to-Action","description":"\u0633\u0645\u062c\u06be\u0646\u0627 \u06a9\u06c1 OpenAI Whisper \u06a9\u06cc\u0633\u06d2 voice-to-action capabilities enable \u06a9\u0631\u062a\u0627 \u06c1\u06d2 humanoid robots \u06a9\u06d2 \u0644\u06cc\u06d2\u060c audio capture \u0633\u06d2 action generation \u062a\u06a9 complete pipeline \u0634\u0627\u0645\u0644 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-4-vision-language-action/voice-to-action"},{"id":"modules/module-4-vision-language-action/introduction","title":"\u062a\u0639\u0627\u0631\u0641 - Vision-Language-Action (VLA) Systems","description":"Vision-Language-Action (VLA) systems \u06a9\u0627 \u062a\u0639\u0627\u0631\u0641\u060c learning objectives\u060c prerequisites\u060c \u0627\u0648\u0631 module structure \u0645\u0627\u0688\u06cc\u0648\u0644 4 \u06a9\u06d2 \u0644\u06cc\u06d2\u06d4","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-4-vision-language-action/introduction"},{"id":"modules/module-4-vision-language-action/module-4-vision-language-action","title":"\u0645\u0627\u0688\u06cc\u0648\u0644 4 - Vision-Language-Action (VLA)","description":"Vision-Language-Action (VLA) systems \u06a9\u0627 \u062a\u0639\u0627\u0631\u0641\u060c LLM-robotics convergence\u060c voice-to-action\u060c cognitive planning\u060c \u0627\u0648\u0631 complete VLA pipeline integration \u06a9\u0627 \u0627\u062d\u0627\u0637\u06c1 \u06a9\u0631\u062a\u06d2 \u06c1\u0648\u0626\u06d2\u06d4","permalink":"/Physical-AI-Humanoid-Robotics-Textbook/ur/modules/module-4-vision-language-action/"}],"unlisted":false}}')}}]);